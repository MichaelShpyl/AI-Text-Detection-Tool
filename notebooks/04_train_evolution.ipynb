{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training (BERT → RoBERTa Evolution)\n",
    "\n",
    "In this notebook, I fine‑tune transformer models on my dataset. I start with **BERT‑base**, then **RoBERTa‑base**, comparing validation performance to select a final model. I also prepare for class imbalance handling and logging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new cwd: c:\\Testing\\Final_Year_Project\\AI-Text-Detection-Tool\n"
     ]
    }
   ],
   "source": [
    "# move up one level so that works\n",
    "import os\n",
    "os.chdir(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "print(\"new cwd:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data sizes → Train: 309520, Val: 38690, Test: 38691\n",
      "Class weights: [1.0000032308194327, 1.0000032308194327, 0.999993538423763]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import logging\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from transformers import TrainingArguments, EarlyStoppingCallback\n",
    "from utils import model_utils\n",
    "\n",
    "# Load configuration\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Load splits\n",
    "train_df = pd.read_parquet(config['paths']['train_data'])\n",
    "val_df   = pd.read_parquet(config['paths']['val_data'])\n",
    "test_df  = pd.read_parquet(config['paths']['test_data'])\n",
    "print(f\"Data sizes → Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "# Set up logging to file\n",
    "logging.basicConfig(\n",
    "    filename=config['paths']['log_file'],\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s: %(message)s\"\n",
    ")\n",
    "logging.info(\"Started training pipeline (BERT → RoBERTa)\")\n",
    "\n",
    "# Compute class weights for loss\n",
    "labels, counts = np.unique(train_df['label'], return_counts=True)\n",
    "class_weights = (1.0 / counts) * np.mean(counts)\n",
    "# Map to [0,1,2] order\n",
    "weight_list = [0]*len(class_weights)\n",
    "for lab, wt in zip(labels, class_weights):\n",
    "    idx = config['model']['label_mapping'][lab]\n",
    "    weight_list[idx] = float(wt)\n",
    "print(\"Class weights:\", weight_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjYAAAGGCAYAAABolMvdAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8ekN5oAAAACXBIWXMAAA9hAAAPYQGoP6dpAABfRUlEQVR4nO3dd1QU19sH8O/Sey+C0uwgYoGoYBSwQKxESdRo7JpgF9QYY+xdoxKjoiYqlhiNNbao/ERskNggFrCjWEACKk2l7M77B4d9swKyi4ury/dzDuewd+7ceWZnIk/uvXNHJAiCACIiIiI1oKHqAIiIiIiUhYkNERERqQ0mNkRERKQ2mNgQERGR2mBiQ0RERGqDiQ0RERGpDSY2REREpDaY2BAREZHaYGJDREREaoOJDb0zly9fxuDBg+Hi4gI9PT0YGRmhefPmWLx4MZ4+fSqt5+fnBz8/P9UFWg6RSCTzY2pqCj8/Pxw6dEjVob2V+fPnY9++faXKY2JiIBKJEBMT885j8vPzg7u7e5UfJy8vD4sWLUKTJk1gYmICY2Nj1KlTB7169cLJkycr1eagQYPg7OwsU+bs7IxBgwZJPz9+/BgzZ85EQkJC5YMnojJpqToAqh5+/vlnjBw5Eg0aNMCkSZPg5uaGwsJCXLhwAWvWrEFcXBz27t2r6jAr9Nlnn2HChAmQSCS4e/cu5s6di27duuHAgQPo0qWLqsOrlPnz5+Ozzz7Dp59+KlPevHlzxMXFwc3NTTWBVTGxWIyAgABcuXIFkyZNQosWLQAAt27dwoEDB3D69Gn4+voq5Vh79+6FiYmJ9PPjx48xa9YsODs7o2nTpko5BhEVY2JDVS4uLg4jRoxAx44dsW/fPujq6kq3dezYERMmTMCRI0dUGKH8bG1t0apVKwCAj48PvL29UbduXYSHh5eb2BQWFkIkEkFL6/36z+3ly5fQ19cvd7uJiYn0XNXRqVOnEBsbiw0bNmDw4MHS8sDAQIwePRoSiURpx2rWrJnS2iKiN+NQFFW5+fPnQyQSYd26dTJJTQkdHR107979jW3MmjULLVu2hIWFBUxMTNC8eXOsX78er7/DNTo6Gn5+frC0tIS+vj4cHR0RHByMFy9eSOtERESgSZMmMDIygrGxMRo2bIjvvvuuUudWp04dWFtb4/79+wD+f/hmy5YtmDBhAmrWrAldXV3cvn0bALBhwwY0adIEenp6sLCwQI8ePZCUlCTT5qBBg2BkZIRr166hffv2MDQ0hLW1NUaPHi1zHgDw6tUrTJkyBS4uLtDR0UHNmjUxatQoPH/+XKaes7Mzunbtij179qBZs2bQ09PDrFmzIBKJkJeXh02bNkmH2EqGAcsbitq/fz+8vb1hYGAAY2NjdOzYEXFxcTJ1Zs6cCZFIhGvXruGLL76AqakpbG1tMWTIEGRlZcn9/Z4+fRqtWrWCvr4+atasiWnTpkEsFgMABEFAvXr1EBgYWGq/3NxcmJqaYtSoUeW2nZmZCQCws7Mrc7uGxv//8xgZGQmRSISoqCgMHjwYFhYWMDQ0RLdu3XD37t0Kz+O/Q1ExMTH46KOPAACDBw+Wfu8zZ86ssB0iqhgTG6pSYrEY0dHR8PT0hIODQ6XbuXfvHr7++mv8/vvv2LNnD3r27IkxY8Zgzpw5MnW6dOkCHR0dbNiwAUeOHMHChQthaGiIgoICAMD27dsxcuRI+Pr6Yu/evdi3bx9CQ0ORl5dXqbiePXuGzMxMWFtby5RPmTIFKSkpWLNmDQ4cOAAbGxssWLAAQ4cORaNGjbBnzx78+OOPuHz5Mry9vXHr1i2Z/QsLC9G5c2e0b98e+/btw+jRo7F27Vr07t1bWkcQBHz66af44Ycf0L9/fxw6dAhhYWHYtGkT2rVrh/z8fJk2L126hEmTJmHs2LE4cuQIgoODERcXB319fXTu3BlxcXGIi4vD6tWryz3fbdu2ISgoCCYmJvjtt9+wfv16PHv2DH5+fjhz5kyp+sHBwahfvz52796Nb7/9Ftu2bUNoaKhc321aWhr69OmDfv364Y8//sBnn32GuXPnYty4cQCK5zyNGTMGUVFRpb6/zZs3Izs7+42JjZeXF7S1tTFu3Dj8+uuvSE1NrTCmoUOHQkNDA9u2bUN4eDjOnTsHPz+/UonkmzRv3hwbN24EAHz//ffS733YsGFyt0FEbyAQVaG0tDQBgNCnTx+59/H19RV8fX3L3S4Wi4XCwkJh9uzZgqWlpSCRSARBEIRdu3YJAISEhIRy9x09erRgZmYmdyz/BUAYOXKkUFhYKBQUFAhJSUlCp06dBADCqlWrBEEQhBMnTggAhLZt28rs++zZM0FfX1/o3LmzTHlKSoqgq6sr9O3bV1o2cOBAAYDw448/ytSdN2+eAEA4c+aMIAiCcOTIEQGAsHjxYpl6O3bsEAAI69atk5Y5OTkJmpqawo0bN0qdl6GhoTBw4MBS5SXncuLECUEQir93e3t7oXHjxoJYLJbWy8nJEWxsbAQfHx9p2YwZM8qMbeTIkYKenp70mpXH19dXACD88ccfMuXDhw8XNDQ0hPv37wuCIAjZ2dmCsbGxMG7cOJl6bm5ugr+//xuPIQiCsH79esHIyEgAIAAQ7OzshAEDBginTp2Sqbdx40YBgNCjRw+Z8rNnzwoAhLlz50rLBg4cKDg5OcnUc3JykvmOz58/LwAQNm7cWGGMRKQY9tjQByE6OhodOnSAqakpNDU1oa2tjenTpyMzMxPp6ekAgKZNm0JHRwdfffUVNm3aVOYQQYsWLfD8+XN88cUX+OOPP5CRkaFQHKtXr4a2tjZ0dHTg6uqK2NhYzJ49GyNHjpSpFxwcLPM5Li4OL1++lHkyBgAcHBzQrl07HD9+vNSx+vXrJ/O5b9++AIATJ05IvxMApdr8/PPPYWhoWKpNDw8P1K9fX74TLcONGzfw+PFj9O/fX2aYxsjICMHBwfjrr79KDZW9PsTo4eGBV69eSa/ZmxgbG5fav2/fvpBIJDh16pS0zuDBgxEZGSntdYuOjkZiYiJGjx5d4TGGDBmChw8fYtu2bRg7diwcHBywdetW+Pr6YsmSJaXqv35NfHx84OTkJL0mRKR6TGyoSllZWcHAwADJycmVbuPcuXMICAgAUPx01dmzZ3H+/HlMnToVQPEkWKB4vsv//vc/2NjYYNSoUahTpw7q1KmDH3/8UdpW//79sWHDBty/fx/BwcGwsbFBy5YtERUVJVcsvXr1wvnz53HhwgXcuHEDmZmZmDZtWql6r8/beNN8Dnt7e+n2ElpaWrC0tJQpq1GjhkxbmZmZ0NLSKjUMJhKJUKNGjVJtljeXRF4VnYNEIsGzZ89kyl8/h5I5ViXX7E1sbW1Llb3+HQDAmDFjkJOTg19//RUAsHLlStSqVQtBQUEVHgMATE1N8cUXX+DHH3/E33//jcuXL8PW1hZTp04tNcRUcvzXy17/rolIdZjYUJXS1NRE+/btcfHiRTx8+LBSbWzfvh3a2to4ePAgevXqBR8fH3h5eZVZt02bNjhw4ACysrLw119/wdvbG+PHj8f27duldQYPHozY2FhkZWXh0KFDEAQBXbt2lU4AfhNra2t4eXnB09MT9evXh6amZpn1RCKRzOeSP/BlzeN4/PgxrKysZMqKiopK/bFMS0uTacvS0hJFRUX4999/ZeoJgoC0tLRSbb4ek6IqOgcNDQ2Ym5u/1TH+68mTJ6XKXv8OAKBu3bro1KkTVq1ahQcPHmD//v0ICQkp99pUpFGjRujTpw8KCwtx8+bNMo//etnrCRwRqQ4TG6pyU6ZMgSAIGD58uHQS738VFhbiwIED5e5f8qj0f/9QvXz5Elu2bCl3H01NTbRs2RKrVq0CUDxx9nWGhobo1KkTpk6dioKCAly7dk2R01KIt7c39PX1sXXrVpnyhw8fIjo6Gu3bty+1T0kPRIlt27YBgPSppZJ9Xm9z9+7dyMvLK7PNsujq6srVg9KgQQPUrFkT27Ztk3kaLS8vD7t375Y+KaUsOTk52L9/v0zZtm3boKGhgbZt28qUjxs3DpcvX8bAgQOhqamJ4cOHV9h+ZmZmmfcjAFy/fh1AcU/Uf71+TWJjY3H//n2FF5RUpOeKiBTzfi2sQWrJ29sbERERGDlyJDw9PTFixAg0atQIhYWFiI+Px7p16+Du7o5u3bqVuX+XLl2wbNky9O3bF1999RUyMzPxww8/lHp0fM2aNYiOjkaXLl3g6OiIV69eYcOGDQCADh06AACGDx8OfX19tG7dGnZ2dkhLS8OCBQtgamoqfQS3KpiZmWHatGn47rvvMGDAAHzxxRfIzMzErFmzoKenhxkzZsjU19HRwdKlS5Gbm4uPPvoIsbGxmDt3Ljp16oSPP/4YQPEaQIGBgZg8eTKys7PRunVrXL58GTNmzECzZs3Qv39/uWJr3LgxYmJicODAAdjZ2cHY2BgNGjQoVU9DQwOLFy9Gv3790LVrV3z99dfIz8/HkiVL8Pz5cyxcuPDtv6j/sLS0xIgRI5CSkoL69evj8OHD+PnnnzFixAg4OjrK1O3YsSPc3Nxw4sQJfPnll7Cxsamw/RMnTmDcuHHo168ffHx8YGlpifT0dPz22284cuQIBgwYgFq1asnsc+HCBQwbNgyff/45Hjx4gKlTp6JmzZql5lhVpE6dOtDX18evv/4KV1dXGBkZwd7evlQiRUSVoNq5y1SdJCQkCAMHDhQcHR0FHR0dwdDQUGjWrJkwffp0IT09XVqvrKeiNmzYIDRo0EDQ1dUVateuLSxYsEBYv369AEBITk4WBEEQ4uLihB49eghOTk6Crq6uYGlpKfj6+gr79++XtrNp0ybB399fsLW1FXR0dAR7e3uhV69ewuXLlyuMH4AwatSoN9YpeZJo586dZW7/5ZdfBA8PD0FHR0cwNTUVgoKChGvXrsnUGThwoGBoaChcvnxZ8PPzE/T19QULCwthxIgRQm5urkzdly9fCpMnTxacnJwEbW1twc7OThgxYoTw7NkzmXpOTk5Cly5dyowpISFBaN26tWBgYCAAkH73rz8VVWLfvn1Cy5YtBT09PcHQ0FBo3769cPbsWZk6JU9F/fvvvzLlJU8XlVyz8vj6+gqNGjUSYmJiBC8vL0FXV1ews7MTvvvuO6GwsLDMfWbOnCkAEP766683tl3iwYMHwvfffy+0bt1aqFGjhqClpSUYGxsLLVu2FH766SehqKioVNzHjh0T+vfvL5iZmUmfcrt165ZMu/I8FSUIgvDbb78JDRs2FLS1tQUAwowZM+SKm4jeTCQIr61wRkQqNWjQIOzatQu5ubmqDuWD4uXlBZFIhPPnzyu97cjISAwePBjnz58vd34XEb0fOBRFRB+s7OxsXL16FQcPHsTFixc/iPeNEVHVYmJDRB+sS5cuwd/fH5aWlpgxY0apF3kSUfXDoSgiIiJSG3zcm4iIiNQGExsiIiJSG0xsiIiISG1Uu8nDEokEjx8/hrGx8VsvMU9ERB8eQRCQk5MDe3t7mRe6knqodonN48eP4eDgoOowiIhIxR48eFBqdWn68FW7xMbY2BhA8Q1tYmKi4miIiOhdy87OhoODg/TvAamXapfYlAw/mZiYMLEhIqrGOB1BPXFwkYiIiNQGExsiIiJSG0xsiIiISG1Uuzk2RESkHGKxGIWFhaoOQ2EFBQVwcnJCQUEBXr16pepwSA46OjpyP5pf7d4VlZ2dDVNTU2RlZXHyMBFRJQiCgLS0NDx//lzVoVSKRCLBgwcP4ODgwHVsPhAaGhpwcXGBjo5OhXXZY0NERAopSWpsbGxgYGDwwT1dJBaL8fLlSzg7O0NTU1PV4VAFShbWTU1NhaOjY4X3GxMbIiKSm1gsliY1lpaWqg6nUsRiMQBAT0+Pic0HwtraGo8fP0ZRURG0tbXfWFelfXCnTp1Ct27dYG9vD5FIhH379lW4z8mTJ+Hp6Qk9PT3Url0ba9asqfpAiYgIAKRzagwMDFQcCVUnJUNQJUnpm6g0scnLy0OTJk2wcuVKueonJyejc+fOaNOmDeLj4/Hdd99h7Nix2L17dxVHSkRE//WhDT/Rh02R+02lQ1GdOnVCp06d5K6/Zs0aODo6Ijw8HADg6uqKCxcu4IcffkBwcHAVRUlEREQfig9qOnhcXBwCAgJkygIDA3HhwoVyHznMz89Hdna2zA8REVFVkXdqBVWND2rycFpaGmxtbWXKbG1tUVRUhIyMDNjZ2ZXaZ8GCBZg1a9a7ClFuvXaMUHUIKvV77whVh6BS1f36A7wHPtR7wELbFH0du0LzuQ40dSr/J6SOhZMSo3q30tLSMG/ePBw6dAiPHj2CjY0NmjZtivHjx6N9+/aqDq/a+6B6bIDS42wly/CUN/42ZcoUZGVlSX8ePHhQ5TESEZF6unfvHjw9PREdHY3FixfjypUrOHLkCPz9/TFq1ChVh0f4wBKbGjVqIC0tTaYsPT0dWlpa5T52qKurK32TN9/oTUREb2PkyJEQiUQ4d+4cPvvsM9SvXx+NGjVCWFgY/vrrrzL3mTx5MurXrw8DAwPUrl0b06ZNk5k+8c8//8Df3x/GxsYwMTGBp6cnLly4AAC4f/8+unXrBnNzcxgaGqJRo0Y4fPjwOznXD9UHNRTl7e2NAwcOyJQdO3YMXl5eFT7XTkRE9DaePn2KI0eOYN68eTA0NCy13czMrMz9jI2NERkZCXt7e1y5cgXDhw+HsbExvvnmGwBAv3790KxZM0REREBTUxMJCQnSv2mjRo1CQUEBTp06BUNDQyQmJsLIyKjKzlEdqDSxyc3Nxe3bt6Wfk5OTkZCQAAsLCzg6OmLKlCl49OgRNm/eDAAICQnBypUrERYWhuHDhyMuLg7r16/Hb7/9pqpTICKiauL27dsQBAENGzZUaL/vv/9e+ruzszMmTJiAHTt2SBOblJQUTJo0SdpuvXr1pPVTUlIQHByMxo0bAwBq1679tqeh9lSa2Fy4cAH+/v7Sz2FhYQCAgQMHIjIyEqmpqUhJSZFud3FxweHDhxEaGopVq1bB3t4eK1as4KPeRERU5Sqa01meXbt2ITw8HLdv30Zubi6KiopkpkWEhYVh2LBh2LJlCzp06IDPP/8cderUAQCMHTsWI0aMwLFjx9ChQwcEBwfDw8NDeSelhlQ6x8bPzw+CIJT6iYyMBABERkYiJiZGZh9fX19cunQJ+fn5SE5ORkhIyLsPnIiIqp169epBJBIhKSlJ7n3++usv9OnTB506dcLBgwcRHx+PqVOnoqCgQFpn5syZuHbtGrp06YLo6Gi4ublh7969AIBhw4bh7t276N+/P65cuQIvLy/89NNPSj83dfJBTR4mIiJSFQsLCwQGBmLVqlXIy8srtb2st52fPXsWTk5OmDp1Kry8vFCvXj3cv3+/VL369esjNDQUx44dQ8+ePbFx40bpNgcHB4SEhGDPnj2YMGECfv75Z6Wel7phYkNERCSn1atXQywWo0WLFti9ezdu3bqFpKQkrFixAt7e3qXq161bFykpKdi+fTvu3LmDFStWSHtjAODly5cYPXo0YmJicP/+fZw9exbnz5+Hq6srAGD8+PE4evQokpOTcenSJURHR0u3Udk+qKeiiIiIVMnFxQWXLl3CvHnzMGHCBKSmpsLa2hqenp6IiCi96GRQUBBCQ0MxevRo5Ofno0uXLpg2bRpmzpwJANDU1ERmZiYGDBiAJ0+ewMrKCj179pQuLCsWizFq1Cg8fPgQJiYm+OSTT7B8+fJ3ecofHCY2RERECrCzs8PKlSvLfYFzySTjEosXL8bixYtlysaPHw+g+K3Vb3qyl/NpFMehKCIiIlIbTGyIiIhIbTCxISIiIrXBxIaIiIjUBhMbIiIiUhtMbIiIiEhtMLEhIiIitcHEhoiIiNQGExsiIiJSG0xsiIiIANy7dw8ikQgJCQmqDuWDExMTA5FIVOaLQN81vlKBiIiUYvy8BAVqK1K3bAeWBr11G//l4OCA1NRUWFlZKbXd91VMTAz8/f3x7NkzmJmZqTocpWGPDREREYpfSFmjRg1oaX3Y/89fUFCg6hBUiokNERFVG0eOHEHbtm3h7+8PGxsbdO3aFXfu3AGg+FDU/v37Ua9ePejr68Pf3x+bNm0qNRwTGxuLtm3bQl9fHw4ODhg7dizy8vKk252dnTF//nwMGTIExsbGcHR0xLp162SO8+jRI/Tu3Rvm5uawtLREUFAQ7t27J90+aNAgfPrpp1iwYAHs7e1Rv359AMDWrVvh5eUFY2Nj1KhRA3379kV6err0XP39/QEA5ubmEIlEGDRoEIDil3guXrwYtWvXhr6+Ppo0aYJdu3bJxHT48GHUr19feu7/jUfVmNgQEVG1kZeXh9DQUGzatAnHjh2DhoYGevToAYlEolA79+7dw2effYZPP/0UCQkJ+PrrrzF16lSZOleuXEFgYCB69uyJy5cvY8eOHThz5gxGjx4tU2/p0qXw8vJCfHw8Ro4ciREjRuD69esAgBcvXsDf3x9GRkY4deoUzpw5AyMjI3zyyScyPTPHjx9HUlISoqKicPDgQQDFPTdz5szBP//8g3379iE5OVmavDg4OGD37t0AgBs3biA1NRU//vgjAOD777/Hxo0bERERgWvXriE0NBRffvklTp48CQB48OABevbsic6dOyMhIQHDhg3Dt99+q9D3V5U+7P42IiIiBQQHB0MsFiM+Ph5NmzbF+vXrYWNjg8TERBgZGcndzpo1a9CgQQMsWbIEANCgQQNcvXoV8+bNk9ZZsmQJ+vbti/HjxwMA6tWrhxUrVsDX1xcRERHQ09MDAHTu3BkjR44EAEyePBnLly9HTEwMGjZsiO3bt0NDQwO//PILRCIRAGDjxo0wMzNDTEwMAgICAACGhob45ZdfoKOjIz3+kCFDpL/Xrl0bK1asQIsWLZCbmwsjIyNYWFgAAGxsbKRzbPLy8rBs2TJER0fD29tbuu+ZM2ewdu1aaey1a9fG8uXLIRKJ0KBBA1y5cgWLFi2S+/urSkxsiIio2rhz5w6+//57nDp1Cjk5OdKempSUFLi5ucndzo0bN/DRRx/JlLVo0ULm88WLF3H79m38+uuv0jJBECCRSJCcnAxXV1cAgIeHh3S7SCRCjRo1pENGJW0YGxvLtP3q1SvpEBoANG7cWCapAYD4+HjMnDkTCQkJePr0qVznmpiYiFevXqFjx44y5QUFBWjWrBkAICkpCa1atZImWgCkSdD7gIkNERFVG926dUOtWrUwdepUtG3bFiKRCO7u7gpPuBUEQeYPe0nZf0kkEnz99dcYO3Zsqf0dHR2lv2tra8tsE4lE0iREIpHA09NTJjkqYW1tLf3d0NBQZlteXh4CAgIQEBCArVu3wtraGikpKQgMDHzjuZYc99ChQ6hZs6bMNl1d3TLP833DxIaIiKqFzMxMJCUlYfXq1TAyMoKrqyvi4uIq1VbDhg1x+PBhmbILFy7IfG7evDmuXbuGunXrVjrm5s2bY8eOHbCxsYGJiYnc+12/fh0ZGRlYuHAhHBwcyoyvpIdHLBZLy9zc3KCrq4uUlBT4+vqW2babmxv27dsnU/bXX3/JHVtV4+RhIiKqFkqeKvrll1/w4MEDREdHIywsrFJtff3117h+/TomT56Mmzdv4vfff0dkZCQASHtyJk+ejLi4OIwaNQoJCQm4desW9u/fjzFjxsh9nH79+sHKygpBQUE4ffo0kpOTcfLkSYwbNw4PHz4sdz9HR0fo6Ojgp59+wt27d7F//37MmTNHpo6TkxNEIhEOHjyIf//9F7m5uTA2NsbEiROlE6zv3LmD+Ph4rFq1Cps2bQIAhISE4M6dOwgLC8ONGzewbds26bm/D5jYEBFRtaChoYHt27fj0qVL6NOnDyZOnCid/KsoFxcX7Nq1C3v27IGHhwciIiKkT0WVDNl4eHjg5MmTuHXrFtq0aYNmzZph2rRpsLOzk/s4BgYGOHXqFBwdHdGzZ0+4urpiyJAhePny5Rt7cKytrREZGYmdO3fCzc0NCxcuxA8//CBTp2bNmpg1axa+/fZb2NraSp/WmjNnDqZPn44FCxbA1dUVgYGBOHDgAFxcXAAUJ027d+/GgQMH0KRJE6xZswbz589X6PurSiLhfR8sU7Ls7GyYmpoiKytLoW49Zeu1Y4TKjv0++L13hKpDUKnqfv0B3gMf6j1goW2Kvo5dUaOWHTR1Kj+boY6FkxKjUkzJU1HNmjWDpqam0tqdN28e1qxZgwcPHiitTSr26tUrJCcnw8XFRfo0WXk4x4aIiKgSVq9ejY8++giWlpY4e/YslixZUmqNGnr3OBRFRET0mpCQEBgZGZX5ExISAgC4desWgoKC4Obmhjlz5mDChAmYOXOmagMn9tgQERG9bvbs2Zg4cWKZ20qmMSxfvhzLly9/l2GRHJjYEBERvcbGxgY2NjaqDoMqgUNRREREpDaY2BAREZHaYGJDREREaoOJDREREakNTh5WE4Ob98JHNZvAQFsfrwrzEffwErb+swdiifiN2wCgc/126OH6CV4WvkTE+S1I+vc2AMBAWx9z2k/EzBPLkZOfq8rTowrw+hPvAaJiTGzUxLFbp7Dtn33IFxfAWNcIod7DENQwAHsS/3zjNlM9E/R064SJR+agtrkjhjbvg4lH5wIA+jXpgQM3/sd/0D4AvP7Ee+Dt3bt3Dy4uLoiPj0fTpk1VHc5bmzlzJvbt24eEhARVhyIXPz8/NG3aFOHh4W/VDhMbNfEoJ03mswABdkY2FW6zNrBAWk46nr/KxpUn12FrZA0AaGBVGzWMrPHzhW3vIHp6W7z+9D7cA6II+V8oeVfumuWrPXW3Elr5fw4ODkhNTYWVlZVS26V3i4mNGglqGICebp2gr62H7Pxc/PrP3gq3peamw8bQEhb6ZnAxd0BK1iNoijQwuFlv/Bi3XlWnQpXA60+8B96OpqYmatSoodIYCgoKoKOjU22PrwycPKxG/rh+DAP3hCL08CxE3T6N56+yK9yWV/ACGy79jkkfh6BL/fZYc34rglwDcf5RAjQ1NDGl7WjM8A/FRzWbqOq0SE68/sR7oGJHjhxB27Zt4e/vDxsbG3Tt2hV37twBUDwUJRKJ5Bq6iYmJgUgkwqFDh9CkSRPo6emhZcuWuHLlirROZmYmvvjiC9SqVQsGBgZo3LgxfvvtN5l2/Pz8MHr0aISFhcHKygodO3YEACxbtgyNGzeGoaEhHBwcMHLkSOTm/v+QYGRkJMzMzLBv3z7Ur18fenp66NixY5kv4NyyZQucnZ1hamqKPn36ICcn562Pf//+fXTr1g3m5uYwNDREo0aNcPjwYen2xMREdO7cGUZGRrC1tUX//v2RkZEh3Z6Xl4cBAwbAyMgIdnZ2WLp0aYXfubyY2KihRzlpuP/8IUa2GCjXtr8eXsKUqIWYHROOQnEhWtRsin3XjyGkRX/sSzqCH86sxeDmvWCobfAuT4MqidefeA+ULy8vD6Ghodi0aROOHTsGDQ0N9OjRAxKJpFLtTZo0CT/88APOnz8PGxsbdO/eHYWFhQCK30jt6emJgwcP4urVq/jqq6/Qv39//P333zJtbNq0CVpaWjh79izWrl0LANDQ0MCKFStw9epVbNq0CdHR0fjmm29k9nvx4gXmzZuHTZs24ezZs8jOzkafPn1k6ty5cwf79u3DwYMHcfDgQZw8eRILFy586+OPGjUK+fn5OHXqFK5cuYJFixbByMgIAJCamgpfX180bdoUFy5cwJEjR/DkyRP06tVL5ns7ceIE9u7di2PHjiEmJgYXL16s1DV4HYei1JSmhibsjK0V3jbM8wtsjP8dYokYzqY1cSvzHookRXj64jlqGFvjztP7VRk2KQmvP1XVPfAs53kVRl31goODIRaLpROE169fDxsbGyQmJkr/MCtixowZ0l6OTZs2oVatWti7dy969eqFmjVryrxvasyYMThy5Ah27tyJli1bSsvr1q2LxYsXy7Q7fvx46e8uLi6YM2cORowYgdWrV0vLCwsLsXLlSmlbmzZtgqurK86dO4cWLVoAACQSCSIjI2FsbAwA6N+/P44fP4558+a91fFTUlIQHByMxo0bAwBq164trR8REYHmzZtj/vz50rINGzbAwcEBN2/ehL29PdavX4/NmzeX+u6UgT02akBXSxd+Lt4w0NYHADiY2iPYrRP+SUt847bX+Tq3wpO8DNzIKO6WfZKXAQ/bhjDXM4WdsQ0y8p6+u5MiufH6E+8B+d25cwdffvklgoKCYG5uDhcXFwDFf6grw9vbW/q7hYUFGjRogKSkJACAWCzGvHnz4OHhAUtLSxgZGeHYsWOljuXl5VWq3RMnTqBjx46oWbMmjI2NMWDAAGRmZiIvL09aR0tLS2bfhg0bwszMTHp8AHB2dpYmNQBgZ2eH9PT0tz7+2LFjMXfuXLRu3RozZszA5cuXpftevHgRJ06ckHkjesOGDQEUf/937txBQUFBmd+dMrDHRh0IAj52/Aj9m/SEtoYWsvJz8ffDePx+9QBEEJW77b+MdAzRrUEHTI/+/3HO9Re3Y0SLAdDT0sXOa4eQlZ/z+pHpfcDrT+/wHrDQNn3XZ6dU3bp1Q61atTB16lS0bdsWIpEI7u7uKCgoUNoxRCIRAGDp0qVYvnw5wsPDpfNVxo8fX+pYhoaGMp/v37+Pzp07IyQkBHPmzIGFhQXOnDmDoUOHSoe5Xj9WeWXa2tqltr0+7FaZ4w8bNgyBgYE4dOgQjh07hgULFmDp0qUYM2YMJBIJunXrhkWLFpWKzc7ODrdu3Srze1MWJjZqIF9cgLknV5S7/U3bSuQW5EnXriiR+O8tjDk07a3jo6rF60+8B+STmZmJpKQkrF69GkZGRnB1dUVcXNxbtfnXX3/B0dERAPDs2TPcvHlT2jtx+vRpBAUF4csvvwRQPCx069YtuLq6vrHNCxcuoKioCEuXLoWGRvHAyu+//16qXlFRES5cuCAddrpx4waeP38uPX5lyXt8BwcHhISEICQkBFOmTMHPP/+MMWPGoHnz5ti9ezecnZ2hpVU6zahbty60tbXL/O58fX3fKnbgPRiKWr16NVxcXKCnpwdPT0+cPn36jfV//fVXNGnSBAYGBrCzs8PgwYORmZn5jqIlIqIPlbm5OSwtLfHLL7/gwYMHiI6ORliY/GvvlGX27Nk4fvw4rl69ikGDBsHKygqffvopgOI/4FFRUYiNjUVSUhK+/vprpKWlvblBAHXq1EFRURF++ukn3L17F1u2bMGaNWtK1dPW1saYMWPw999/49KlSxg8eDBatWolTXQqS57jjx8/HkePHkVycjIuXbqE6OhoacI2atQoPH36FF988QXOnTuHu3fv4tixYxgyZAjEYjGMjIwwdOhQTJo0Sea7K0mi3pZKE5sdO3Zg/PjxmDp1KuLj49GmTRt06tSp3LHOM2fOYMCAARg6dCiuXbuGnTt34vz58xg2bNg7jpyIiD40Ghoa2L59Oy5duoQ+ffpg4sSJWLJkyVu1uXDhQowbNw6enp5ITU3F/v37pevATJs2Dc2bN0dgYCD8/PxQo0YNadLzJk2bNsWyZcuwaNEiuLu749dff8WCBQtK1TMwMMDkyZPRt29feHt7Q19fH9u3b3+r85H3+GKxGKNGjYKrqys++eQTNGjQQDqx2N7eHmfPnoVYLEZgYCDc3d0xbtw4mJqaSpOXJUuWoG3btujevTs6dOiAjz/+GJ6enm8dOwCIBEEQlNJSJbRs2RLNmzdHRESEtMzV1RWffvppmRfxhx9+QEREhHTNAQD46aefsHjx4jKf3S9LdnY2TE1NkZWVBRMTk7c/iUrqtWOEyo79Pvi9d0TFldRYdb/+AO+BD/UesNA2RV/HrqhRyw6aOpWfzVDHwkmJUSmm5KmoZs2aQVNTs1JtxMTEwN/fH8+ePYOZmZlyA5RDZGQkxo8fj+fPn7/zY6vCq1evkJycLB3heROV9dgUFBTg4sWLCAgIkCkPCAhAbGxsmfv4+Pjg4cOHOHz4MARBwJMnT7Br1y506dLlXYRMRERE7zmVJTYZGRkQi8WwtbWVKbe1tS13DNLHxwe//vorevfuDR0dHdSoUQNmZmb46aefyj1Ofn4+srOzZX6IiIjeJCQkROZx5f/+hISEqDo8eoO3fipKLBbjypUrcHJygrm5ucL7v/6omiAIZT6+BhQv0Tx27FhMnz4dgYGBSE1NxaRJkxASEoL168t+p8mCBQswa9YsheMiIqLqa/bs2TKL6/2XiYkJbGxsoMKZHBg0aBAGDRqksuO/zxRObMaPH4/GjRtj6NChEIvF8PX1RWxsLAwMDHDw4EH4+fnJ1Y6VlRU0NTVL9c6kp6eX6sUpsWDBArRu3RqTJk0CAHh4eMDQ0BBt2rTB3LlzYWdnV2qfKVOmyMx6z87OhoODg5xnS0RE1ZGNjQ1sbGxUHQZVgsJDUbt27UKTJsUvQztw4ACSk5Nx/fp16dNN8tLR0YGnpyeioqJkyqOiouDj41PmPi9evCj1OFjJxK/yMmddXV2YmJjI/BAREZF6UjixycjIkL7W/fDhw/j8889Rv359DB06VOatpvIICwvDL7/8gg0bNiApKQmhoaFISUmRjl9OmTIFAwYMkNbv1q0b9uzZg4iICNy9exdnz57F2LFj0aJFC9jb2yt6KkRERKRmFB6KsrW1RWJiIuzs7HDkyBHpc+svXrxQ+LG53r17IzMzE7Nnz0Zqairc3d1x+PBhODkVPwaYmpoqs6bNoEGDkJOTg5UrV2LChAkwMzNDu3btyly2mYiIiKofhRObwYMHo1evXrCzs4NIJJK+mfPvv/+u1DLOI0eOxMiRI8vcFhkZWapszJgxGDNmjMLHISIiIvWncGIzc+ZMuLu748GDB/j888+hq6sLoHiuy7fffqv0AImIiIjkVanHvT/77DMAxSsBlhg4cKByIiIiIqoCfn5+aNq0KZYuXVpx5WpG1SspK5PCiY1YLMb8+fOxZs0aPHnyBDdv3kTt2rUxbdo0ODs7Y+jQoVURJxERveemRC18p8er7q/lUCYfHx+kpqbC1NQUQPmvbHB2dsb48eMxfvz4dx+knBR+KmrevHmIjIzE4sWLpS/6AoDGjRvjl19+UWpwREREVLUKCwulq/mXt0Duh0ThxGbz5s1Yt24d+vXrJ/MUlIeHB65fv67U4IiIiJRJIpFg8uTJaN++PWrWrImZM2cCAO7duweRSISEhARp3efPn0MkEiEmJgZA8XCNSCTC0aNH0axZM+jr66Ndu3ZIT0/Hn3/+CVdXV5iYmOCLL77AixcvpO0cOXIEH3/8MczMzGBpaYmuXbvKvMy55Nh79uyBv78/DAwM0KRJE8TFxVV4PoIgwNraGrt375aWNW3aVGZxwbi4OGhrayM3NxdA8Yr/a9asQVBQEAwNDTF37lzpuT1//hwxMTEYPHgwsrKyIBKJIBKJMHPmTPj5+eH+/fsIDQ2VlpeIjY1F27Ztoa+vDwcHB4wdOxZ5eXnS7c7Ozpg/fz6GDBkCY2NjODo6Yt26dfJdNAUpnNg8evQIdevWLVUukUhQWFiolKCIiIiqwqZNm2BoaIiNGzdi4cKFmD17dqmFYisyc+ZMrFy5ErGxsXjw4AF69eqF8PBwbNu2DYcOHUJUVJTMOwzz8vIQFhaG8+fP4/jx49DQ0ECPHj0gkUhk2p06dSomTpyIhIQE1K9fH1988QWKioreGItIJELbtm2lydezZ8+QmJiIwsJCJCYmAihOyDw9PWFkZCTdb8aMGQgKCsKVK1cwZMgQmTZ9fHwQHh4OExMTpKamIjU1FRMnTsSePXtQq1Yt6RItqampAIArV64gMDAQPXv2xOXLl7Fjxw6cOXMGo0ePlml36dKl8PLyQnx8PEaOHIkRI0ZUSYeIwnNsGjVqhNOnT0vXmimxc+dONGvWTGmBERERKZuHhwemT5+O+Ph4NGvWDKtXr8bx48dRr149uduYO3cuWrduDQAYOnQopkyZgjt37qB27doAih+wOXHiBCZPngwACA4Oltl//fr1sLGxQWJiItzd3aXlEydORJcuXQAAs2bNQqNGjXD79u0Kl1Lx8/OT9n6cOnUKTZo0gaOjI2JiYuDm5oaYmJhSrzvq27evTEKTnJws/V1HRwempqYQiUTSBXlLaGpqwtjYWKZ8yZIl6Nu3r3TeTb169bBixQr4+voiIiICenp6AIDOnTtLl3eZPHkyli9fjpiYmEotFfMmCvfYzJgxA6NHj8aiRYsgkUiwZ88eDB8+HPPnz8f06dOVGhwREZEyeXh4yHy2s7NDenp6pduwtbWFgYGBNKkpKftvm3fu3EHfvn1Ru3ZtmJiYwMXFBQBkFqB9vd2Sdx/KE5ufnx+uXbuGjIwMnDx5En5+fvDz88PJkydRVFSE2NhY+Pr6yuzj5eWlwBm/2cWLFxEZGSnzBvTAwEBIJBKZhOm/51eSNCn63ctD4R6bbt26YceOHZg/fz5EIhGmT5+O5s2b48CBA9LF+oiIiN5H2traMp9FIhEkEon0PYT/fe9gedMr/tuGSCQqt80S3bp1g4ODA37++WfY29tDIpHA3d0dBQUFb2wXQKnhqrK4u7vD0tISJ0+exMmTJzF79mw4ODhg3rx5OH/+PF6+fImPP/5YZh9DQ8MK25WXRCLB119/jbFjx5ba5ujoKP29ou9JWSq1jk1gYCACAwOVHQsREZFKWFtbAyh+lU/JtIr/TiSurMzMTCQlJWHt2rVo06YNAODMmTNv3e5/lcyz+eOPP3D16lW0adMGxsbGKCwsxJo1a9C8eXMYGxsr1KaOjg7EYrFc5c2bN8e1a9fKnH+rCgoPRREREakbfX19tGrVCgsXLkRiYiJOnTqF77///q3bNTc3h6WlJdatW4fbt28jOjoaYWFhSohYlp+fH7Zt2wYPDw+YmJhIk51ff/211PwaeTg7OyM3NxfHjx9HRkaG9CkvZ2dnnDp1Co8ePUJGRgaA4vkycXFxGDVqFBISEnDr1i3s379fZa8/kiuxMTc3h4WFhVw/REREH6INGzagsLAQXl5eGDduHObOnfvWbWpoaGD79u24ePEi3N3dERoaiiVLlighWln+/v4Qi8UySYyvry/EYnGp+TXy8PHxQUhICHr37g1ra2ssXrwYADB79mzcu3cPderUkfZyeXh44OTJk7h16xbatGmDZs2aYdq0adJ5Qu+aSPjvgGI5Nm3aJHeD7/urFbKzs2FqaoqsrCyYmJioLI5eO0ao7Njvg+q+Ymh1v/4A74EP9R6w0DZFX8euqFHLDpo6lZrNAACoY+FUcaUqIhaLpU9F/Xc9Nnp/vXr1CsnJyXBxcZE+ZVUeue7K9z1ZISIiIgIqOXlYLBZj7969SEpKgkgkgqurK4KCgqClVfnsnYiIiErr1KkTTp8+Xea27777Dt999907juj9pnAmcvXqVQQFBSEtLQ0NGjQAANy8eRPW1tbYv38/GjdurPQgiYiIqqtffvkFL1++LHMb57aWpnBiM2zYMDRq1AgXLlyAubk5gOIlnAcNGoSvvvpKrndbEBERkXxq1qyp6hA+KAonNv/8849MUgMUPzU1b948fPTRR0oNjoiIiEgRCq9j06BBAzx58qRUeXp6+nuzOA8REVUNARU+SEukdHI8wC2lcGIzf/58jB07Frt27cLDhw/x8OFD7Nq1C+PHj8eiRYuQnZ0t/SEiIvWSV/QSRRIxxPlvfus0kTKVvH5CnsfzFR6K6tq1KwCgV69e0ndZlGRS3bp1k34WiURlLsdMREQfrgKhEP88vw5dLR1YwAKaupV7GvbVq1dKjkx+JX+bXr16xXVsPgASiQT//vsvDAwM5Hr6WuE78sSJE5UKjIiI1EPcswQAQJOihtDSqFxiIH5WUHGlKiKRSJCRkYF79+5JX35J7zcNDQ04OjpKO1TeROHEpjJLMxMRkXqJe5aAi8+vwVBLHyJU/MfmdeGdZyo/KDnl5uaiS5cuuHDhAoyMjFQWB8lPR0dH7iS0Un2Ir169wuXLl5Genl7qlePdu3evTJNERPSBKRAKUVBYWKl9K1oWvyoVFBTg/v370NHRUWkcVDUUTmyOHDmCAQMGSN/q+V+cV0NERESqpPDg4ujRo/H5558jNTUVEolE5odJDREREamSwolNeno6wsLCYGtrWxXxEBEREVWawonNZ599hpiYmCoIhYiIiOjtKDzHZuXKlfj8889x+vRpNG7cGNra2jLbx44dq7TgiIiIiBShcGKzbds2HD16FPr6+oiJiZF5plwkEjGxISIiIpVROLH5/vvvMXv2bHz77bdc2OgtLLydruoQSIV4/Yn3AFHVUDgzKSgoQO/evZnUEBER0XtH4exk4MCB2LFjR1XEQkRERPRWFB6KEovFWLx4MY4ePQoPD49Sk4eXLVumtOCIiIiIFKFwYnPlyhU0a9YMAHD16lWZbfK8nIqIiIioqvDt3kRERKQ2OAOYiIiI1Eal3u59/vx57Ny5EykpKSgoKJDZtmfPHqUERkRERKQohXtstm/fjtatWyMxMRF79+5FYWEhEhMTER0dDVNT06qIkYiIiEguCic28+fPx/Lly3Hw4EHo6Ojgxx9/RFJSEnr16gVHR8eqiJGIiIhILgonNnfu3EGXLl0AALq6usjLy4NIJEJoaCjWrVun9ACJiIiI5KVwYmNhYYGcnBwAQM2aNaWPfD9//hwvXrxQbnREREREClB48nCbNm0QFRWFxo0bo1evXhg3bhyio6MRFRWF9u3bV0WMRERERHJROLFZuXIlXr16BQCYMmUKtLW1cebMGfTs2RPTpk1TeoBERERE8qrUUJS9vX3xzhoa+Oabb7B//34sW7YM5ubmCgewevVquLi4QE9PD56enjh9+vQb6+fn52Pq1KlwcnKCrq4u6tSpgw0bNih8XCIiIlI/CvfYXLp0Cdra2mjcuDEA4I8//sDGjRvh5uaGmTNnQkdHR+62duzYgfHjx2P16tVo3bo11q5di06dOiExMbHcJ6x69eqFJ0+eYP369ahbty7S09NRVFSk6GkQERGRGlK4x+brr7/GzZs3AQB3795F7969YWBggJ07d+Kbb75RqK1ly5Zh6NChGDZsGFxdXREeHg4HBwdERESUWf/IkSM4efIkDh8+jA4dOsDZ2RktWrSAj4+PoqdBREREakjhxObmzZto2rQpAGDnzp3w9fXFtm3bEBkZid27d8vdTkFBAS5evIiAgACZ8oCAAMTGxpa5z/79++Hl5YXFixejZs2aqF+/PiZOnIiXL18qehpERESkhhQeihIEARKJBADwv//9D127dgUAODg4ICMjQ+52MjIyIBaLYWtrK1Nua2uLtLS0Mve5e/cuzpw5Az09PezduxcZGRkYOXIknj59Wu48m/z8fOTn50s/Z2dnyx0jERERfVgU7rHx8vLC3LlzsWXLFpw8eVK6WF9ycnKpJEUeIpFI5rMgCKXKSkgkEohEIvz6669o0aIFOnfujGXLliEyMrLcXpsFCxbA1NRU+uPg4KBwjERERPRhUDixCQ8Px6VLlzB69GhMnToVdevWBQDs2rVLobkuVlZW0NTULNU7k56eXm6CZGdnh5o1a8q8k8rV1RWCIODhw4dl7jNlyhRkZWVJfx48eCB3jERERPRhUXgoysPDA1euXClVvmTJEmhqasrdjo6ODjw9PREVFYUePXpIy6OiohAUFFTmPq1bt8bOnTuRm5sLIyMjAMVzfjQ0NFCrVq0y99HV1YWurq7ccREREdGHS+Eem/Lo6elBW1tboX3CwsLwyy+/YMOGDUhKSkJoaChSUlIQEhICoLi3ZcCAAdL6ffv2haWlJQYPHozExEScOnUKkyZNwpAhQ6Cvr6+sUyEiIqIPlMI9NsrUu3dvZGZmYvbs2UhNTYW7uzsOHz4MJycnAEBqaipSUlKk9Y2MjBAVFYUxY8bAy8sLlpaW6NWrF+bOnauqUyAiIqL3iEoTGwAYOXIkRo4cWea2yMjIUmUNGzZEVFRUFUdFREREHyK5hqL4iDQRERF9CORKbMzNzZGeng4AaNeuHZ4/f16VMRERERFVilyJjZGRETIzMwEAMTExKCwsrNKgiIiIiCpDrjk2HTp0gL+/P1xdXQEAPXr0KPdll9HR0cqLjoiIiEgBciU2W7duxaZNm3Dnzh2cPHkSjRo1goGBQVXHRkRERKQQuRIbfX196doyFy5cwKJFi2BmZlaVcREREREpTOHHvU+cOCH9XRAEAKXf90RERESkCpVaeXjz5s1o3Lgx9PX1oa+vDw8PD2zZskXZsREREREpROEem2XLlmHatGkYPXo0WrduDUEQcPbsWYSEhCAjIwOhoaFVEScRERFRhRRObH766SdERETIvMMpKCgIjRo1wsyZM5nYEBERkcooPBSVmpoKHx+fUuU+Pj5ITU1VSlBERERElaFwYlO3bl38/vvvpcp37NiBevXqKSUoIiIiospQeChq1qxZ6N27N06dOoXWrVtDJBLhzJkzOH78eJkJDxEREdG7onCPTXBwMP7++29YWVlh37592LNnD6ysrHDu3Dn06NGjKmIkIiIikovCPTYA4Onpia1btyo7FiIiIqK3Uql1bIiIiIjeR5XqsaH3j2XAUBg2aAENXQNICl4iLykOmce3AJKiN24DAJOPusC8dTAk+S/w78HVePUgEQCgoWsA+4Hz8XjrdEheZKvy9KgCvP7Ee4CoGBMbNZF98QientgKoTAfGgYmsO0xAWbeQXh+dvcbt2kamsH848/w8OdQ6NaoA6tPhuHhz2EAAIt2/fH8rz/4D9oHgNefeA8QFeNQlJoozHwEoTD//wsECbQt7CrcpmVqjcKnqRDnPsfL5MvQMq8BANCt1QDa5jWQe/n/3w1G7y9ef+I9QFSMPTZqxNS7B8xbB0NDVx/iF9nIPLG1wm2FT1OhZWYDTWML6NaojYL0+4CGJqwChiJ933JVnQpVAq8/8R4gAkRCySu65ZSXl4eFCxfi+PHjSE9Ph0Qikdl+9+5dpQaobNnZ2TA1NUVWVhZMTExUFsfdecFV1ra2ZU0YubdF9qWjEOc8rXCbYUNvmPl8Ckn+S2Qc2wDD+h8BIhHyrv8Nyw4DINLSQdb5Q3hx45zSYqw9dbfS2voQVffrD/AeqO73gCqv//vyd4CqhsI9NsOGDcPJkyfRv39/2NnZQSQSVUVc9BYKMx+h4Mk9WHcbg7Rtsyrclnc9DnnX4wAAWuZ2MGzQEo8iv4P9gDl4enwLCv69j1rDl+Hh/WuQvMp75+dDiuH1J94DVJ0pnNj8+eefOHToEFq3bl0V8ZCyaGpKx9AV2Wb1yXBkHNsASIqgY+OEV49vAuIiFGVnQtvCDvmPb1dh0KQ0vP7Ee4CqKYUnD5ubm8PCwqIqYqFKEmnrwcjDHxq6BgAAbWtHmLf+DC/vJrxx2+uMGvuh6PkT5D+8DgAoep4OA5cm0DQyh7aFPYqy/n1n50Ty4/Un3gNE/0/hHps5c+Zg+vTp2LRpEwwMDKoiJlKYAKNGbWDZfiBEWloQ52Uj7/pfeHZqOyASlb/tPzT0jWDWqjseb/5eWpZx5GdYdx0FDR09PDu9A+K8rHd9YiQXXn/iPUBUQuHJw82aNcOdO3cgCAKcnZ2hra0ts/3SpUtKDVDZ3pdJY1U5cfBDwImj1fv6A7wHqvs9wMnDVFUU7rH59NNPqyAMIiIiorencGIzY8aMqoiDiIiI6K1VeoG+ixcvIikpCSKRCG5ubmjWrJky4yIiIiJSmMKJTXp6Ovr06YOYmBiYmZlBEARkZWXB398f27dvh7W1dVXESURERFQhhR/3HjNmDLKzs3Ht2jU8ffoUz549w9WrV5GdnY2xY8dWRYxEREREclG4x+bIkSP43//+B1dXV2mZm5sbVq1ahYCAAKUGR0RERKQIhXtsJBJJqUe8AUBbW7vUe6OIiIiI3iWFE5t27dph3LhxePz4sbTs0aNHCA0NRfv27ZUaHBEREZEiFE5sVq5ciZycHDg7O6NOnTqoW7cuXFxckJOTg59++qkqYiQiIiKSi8JzbBwcHHDp0iVERUXh+vXrEAQBbm5u6NChQ1XER0RERCS3Sq9j07FjR3Ts2FGZsRARERG9FbkSmxUrVuCrr76Cnp4eVqxY8ca6fOSbiIiIVEWuxGb58uXo168f9PT0sHz58nLriUQiJjZERESkMnIlNsnJyWX+TkRERPQ+UfipqNmzZ+PFixelyl++fInZs2crJSgiIiKiylA4sZk1axZyc3NLlb948QKzZs1SSlBERERElaFwYiMIAkQiUanyf/75BxYWFkoJioiIiKgy5H7c29zcHCKRCCKRCPXr15dJbsRiMXJzcxESElIlQRIRERHJQ+7EJjw8HIIgYMiQIZg1axZMTU2l23R0dODs7Axvb2+FA1i9ejWWLFmC1NRUNGrUCOHh4WjTpk2F+509exa+vr5wd3dHQkKCwsclIiIi9SN3YjNw4EAUFRUBADp06IBatWq99cF37NiB8ePHY/Xq1WjdujXWrl2LTp06ITExEY6OjuXul5WVhQEDBqB9+/Z48uTJW8dBRERE6kGhOTZaWloYOXIkxGKxUg6+bNkyDB06FMOGDYOrqyvCw8Ph4OCAiIiIN+739ddfo2/fvpXqISIiIiL1pfDk4ZYtWyI+Pv6tD1xQUICLFy8iICBApjwgIACxsbHl7rdx40bcuXMHM2bMeOsYiIiISL0o/K6okSNHYsKECXj48CE8PT1haGgos93Dw0OudjIyMiAWi2FraytTbmtri7S0tDL3uXXrFr799lucPn0aWlryhZ6fn4/8/Hzp5+zsbLn2IyIiog+PwolN7969Aci+E0okEkkfA1d0mOr1R8fLe5xcLBajb9++mDVrFurXry93+wsWLOD6OkRERNWEwomNsl6pYGVlBU1NzVK9M+np6aV6cQAgJycHFy5cQHx8PEaPHg0AkEgkEAQBWlpaOHbsGNq1a1dqvylTpiAsLEz6OTs7Gw4ODko5ByIiInq/KJzYODk5KeXAOjo68PT0RFRUFHr06CEtj4qKQlBQUKn6JiYmuHLlikzZ6tWrER0djV27dsHFxaXM4+jq6kJXV1cpMRMREdH7TeHEBgDu3LmD8PBwJCUlQSQSwdXVFePGjUOdOnUUaicsLAz9+/eHl5cXvL29sW7dOqSkpEgX+psyZQoePXqEzZs3Q0NDA+7u7jL729jYQE9Pr1Q5ERERVU8KJzZHjx5F9+7d0bRpU7Ru3RqCICA2NhaNGjXCgQMH0LFjR7nb6t27NzIzMzF79mykpqbC3d0dhw8flvYKpaamIiUlRdEQiYiIqJoSCYIgKLJDs2bNEBgYiIULF8qUf/vttzh27BguXbqk1ACVLTs7G6ampsjKyoKJiYnK4rg7L1hlx34f1J66W9UhqFR1v/4A74Hqfg+o8vq/L38HqGoovI5NUlIShg4dWqp8yJAhSExMVEpQRERERJWhcGJjbW1d5ruZEhISYGNjo4yYiIiIiCpF4Tk2w4cPx1dffYW7d+/Cx8cHIpEIZ86cwaJFizBhwoSqiJGIiIhILgonNtOmTYOxsTGWLl2KKVOmAADs7e0xc+ZMmUX7iIiIiN41hRMbkUiE0NBQhIaGIicnBwBgbGys9MCIiIiIFFWpdWyA4hWCb9y4AZFIhAYNGsDa2lqZcREREREpTOHJw9nZ2ejfvz/s7e3h6+uLtm3bwt7eHl9++SWysrKqIkYiIiIiuSic2AwbNgx///03Dh06hOfPnyMrKwsHDx7EhQsXMHz48KqIkYiIiEguCg9FHTp0CEePHsXHH38sLQsMDMTPP/+MTz75RKnBERERESlC4R4bS0tLmJqalio3NTWFubm5UoIiIiIiqgyFE5vvv/8eYWFhSE1NlZalpaVh0qRJmDZtmlKDIyIiIlKEwkNRERERuH37NpycnODo6AgASElJga6uLv7991+sXbtWWvd9f28UERERqReFE5tPP/20CsKofsY9HaDqEFTqgKoDULHqfv0B3gPV/R6o7tefqo7Cic2MGTOqIg4iIiKit1bpBfouXryIpKQkiEQiuLm5oVmzZsqMi4iIiEhhCic26enp6NOnD2JiYmBmZgZBEJCVlQV/f39s376dKxATERGRyij8VNSYMWOQnZ2Na9eu4enTp3j27BmuXr2K7OxsvgSTiIiIVErhHpsjR47gf//7H1xdXaVlbm5uWLVqFQICApQaHBEREZEiFO6xkUgk0NbWLlWura0NiUSilKCIiIiIKkPhxKZdu3YYN24cHj9+LC179OgRQkND0b59e6UGR0RERKQIhROblStXIicnB87OzqhTpw7q1q0LFxcX5OTk4KeffqqKGImIiIjkovAcGwcHB1y6dAlRUVG4fv06BEGAm5sbOnToUBXxEREREclNocSmqKgIenp6SEhIQMeOHdGxY8eqiouIiIhIYQoNRWlpacHJyQlisbiq4iEiIiKqtEq93XvKlCl4+vRpVcRDREREVGkKz7FZsWIFbt++DXt7ezg5OcHQ0FBmO9/oTURERKqicGITFBQEkUhUFbEQERERvRWFE5uZM2dWQRhEREREb0/uOTYvXrzAqFGjULNmTdjY2KBv377IyMioytiIiIiIFCJ3YjNjxgxERkaiS5cu6NOnD6KiojBixIiqjI2IiIhIIXIPRe3Zswfr169Hnz59AABffvklWrduDbFYDE1NzSoLkIiIiEhecvfYPHjwAG3atJF+btGiBbS0tGTeGUVERESkSnInNmKxGDo6OjJlWlpaKCoqUnpQRERERJUh91CUIAgYNGgQdHV1pWWvXr1CSEiIzFo2e/bsUW6ERERERHKSO7EZOHBgqbIvv/xSqcEQERERvQ25E5uNGzdWZRxEREREb03hd0URERERva+Y2BAREZHaYGJDREREaoOJDREREakNJjZERESkNpjYEBERkdpgYkNERERqg4kNERERqQ2VJzarV6+Gi4sL9PT04OnpidOnT5dbd8+ePejYsSOsra1hYmICb29vHD169B1GS0RERO8zlSY2O3bswPjx4zF16lTEx8ejTZs26NSpE1JSUsqsf+rUKXTs2BGHDx/GxYsX4e/vj27duiE+Pv4dR05ERETvI5UmNsuWLcPQoUMxbNgwuLq6Ijw8HA4ODoiIiCizfnh4OL755ht89NFHqFevHubPn4969erhwIED7zhyIiIieh+pLLEpKCjAxYsXERAQIFMeEBCA2NhYudqQSCTIycmBhYVFuXXy8/ORnZ0t80NERETqSWWJTUZGBsRiMWxtbWXKbW1tkZaWJlcbS5cuRV5eHnr16lVunQULFsDU1FT64+Dg8FZxExER0ftL5ZOHRSKRzGdBEEqVleW3337DzJkzsWPHDtjY2JRbb8qUKcjKypL+PHjw4K1jJiIioveTlqoObGVlBU1NzVK9M+np6aV6cV63Y8cODB06FDt37kSHDh3eWFdXVxe6urpvHS8RERG9/1TWY6OjowNPT09ERUXJlEdFRcHHx6fc/X777TcMGjQI27ZtQ5cuXao6TCIiIvqAqKzHBgDCwsLQv39/eHl5wdvbG+vWrUNKSgpCQkIAFA8jPXr0CJs3bwZQnNQMGDAAP/74I1q1aiXt7dHX14epqanKzuN98FWPxmjlbgdDPS28zC/CmX8eI/LgNRSJhTduA4DubWrj8/b18SK/ECt2JODa3UwAgKGeFhaPaYMpq88iO69AladHFeD1J94DRMVUmtj07t0bmZmZmD17NlJTU+Hu7o7Dhw/DyckJAJCamiqzps3atWtRVFSEUaNGYdSoUdLygQMHIjIy8l2H/145fDYZmw4lIr9ADBNDHUwe4IWe/vXw+/9uvnGbmbEuenesj9FLTqBuLTOE9PTAmB9OAAAGdW2EPTF3+A/aB4DXn3gPEBVTaWIDACNHjsTIkSPL3PZ6shITE1P1AX2gHqbnynwWBMDeyrDCbTbmBnj8bx6e5eQj4da/mGxpAABwdbaAnZUhVu365x1ET2+L1594DxAVU3liQ8rzWbt6+Lx9fRjoaSE7Lx+RBxMr3Pb431zYWhjA0lQPtWua4l5qDjQ1RPjq08ZYsvWCqk6FKoHXn3gPEAEiQRAEVQfxLmVnZ8PU1BRZWVkwMTFRWRzdJvxRZW3XsjGCX/Na+DPuHjKzXlW4rbWHPYLb1cOLV4X4ed8VtHS3gwhA3JVUDOnWCDramth/+i7+upqqtBgPLA1SWlsfoup+/QHeA9X9HlDl9X9f/g5Q1VD5OjakfA/Tc5H8OBvj+zSXa9vZy48RFn4S36+JRUGRBN7udth94hbG9G6KndG3MC/yHL7q0RiG+trv8jSoknj9ifcAVWdMbNSUlqZIOoauyLYRwR74+Y8rKBILcLE3xY37z5D3shCZz1+Wuw+9f3j9ifcAVVdMbNSAno4m2n/kCEO94ilTTjWM0atDA1y6kf7Gba9r5+WAJ5kvkJj8FADwJDMPzepbw8JED/bWRvj32ct3d1IkN15/4j1A9P84eVgNCAB8m9fEkG6NoK2lgazcfMRefoxtR28Aojds+w9jA2308KuLb1eelpZF7LmMcb2bQU9HC78du47nufnv+MxIHrz+xHuA6P9x8rCKVOXEwQ8BJ45W7+sP8B6o7vcAJw9TVeFQFBEREakNJjZERESkNpjYEBERkdpgYkNERERqg4kNERERqQ0mNkRERKQ2mNgQERGR2mBiQ0RERGqDiQ0RERGpDSY2REREpDaY2BAREZHaYGJDREREaoOJDREREakNJjZERESkNpjYEBERkdpgYkNERERqg4kNERERqQ0mNkRERKQ2mNgQERGR2mBiQ0RERGqDiQ0RERGpDSY2REREpDaY2BAREZHaYGJDREREaoOJDREREakNJjZERESkNpjYEBERkdpgYkNERERqg4kNERERqQ0mNkRERKQ2mNgQERGR2mBiQ0RERGqDiQ0RERGpDSY2REREpDaY2BAREZHaYGJDREREaoOJDREREakNlSc2q1evhouLC/T09ODp6YnTp0+/sf7Jkyfh6ekJPT091K5dG2vWrHlHkRIREdH7TqWJzY4dOzB+/HhMnToV8fHxaNOmDTp16oSUlJQy6ycnJ6Nz585o06YN4uPj8d1332Hs2LHYvXv3O46ciIiI3kcqTWyWLVuGoUOHYtiwYXB1dUV4eDgcHBwQERFRZv01a9bA0dER4eHhcHV1xbBhwzBkyBD88MMP7zhyIiIieh9pqerABQUFuHjxIr799luZ8oCAAMTGxpa5T1xcHAICAmTKAgMDsX79ehQWFkJbW7vUPvn5+cjPz5d+zsrKAgBkZ2e/7Sm8lcL8Fyo9vqqp+vtXtep+/QHeA9X9HlDl9S85tiAIKouBqo7KEpuMjAyIxWLY2trKlNva2iItLa3MfdLS0sqsX1RUhIyMDNjZ2ZXaZ8GCBZg1a1apcgcHh7eInt6W6SpVR0Cqxnugensfrn9OTg5MTU1VHQYpmcoSmxIikUjmsyAIpcoqql9WeYkpU6YgLCxM+lkikeDp06ewtLR843HUWXZ2NhwcHPDgwQOYmJioOhx6x3j9qbrfA4IgICcnB/b29qoOhaqAyhIbKysraGpqluqdSU9PL9UrU6JGjRpl1tfS0oKlpWWZ++jq6kJXV1emzMzMrPKBqxETE5Nq+Y8aFeP1p+p8D7CnRn2pbPKwjo4OPD09ERUVJVMeFRUFHx+fMvfx9vYuVf/YsWPw8vIqc34NERERVS8qfSoqLCwMv/zyCzZs2ICkpCSEhoYiJSUFISEhAIqHkQYMGCCtHxISgvv37yMsLAxJSUnYsGED1q9fj4kTJ6rqFIiIiOg9otI5Nr1790ZmZiZmz56N1NRUuLu74/Dhw3BycgIApKamyqxp4+LigsOHDyM0NBSrVq2Cvb09VqxYgeDgYFWdwgdJV1cXM2bMKDVER9UDrz/xHiB1JhL4vBsRERGpCZW/UoGIiIhIWZjYEBERkdpgYkNERERqg4kNERERqQ0mNkRERKQ2VP5KBXo3nj9/jnPnziE9PR0SiURm23/XCiIi9XPq1Cn4+PhAS0v2n/yioiLExsaibdu2KoqMSPn4uHc1cODAAfTr1w95eXkwNjaWeUeWSCTC06dPVRgdVZUVK1bIXXfs2LFVGAmpmqamJlJTU2FjYyNTnpmZCRsbG4jFYhVFRqR8TGyqgfr166Nz586YP38+DAwMVB0OvSMuLi5y1ROJRLh7924VR0OqpKGhgSdPnsDa2lqm/ObNm/Dy8kJ2draKIiNSPg5FVQOPHj3C2LFjmdRUM8nJyaoOgVSsZ8+eAIqT10GDBsmsNCwWi3H58uVy381H9KFiYlMNBAYG4sKFC6hdu7aqQyGid6jkDdaCIMDY2Bj6+vrSbTo6OmjVqhWGDx+uqvCIqgQTm2qgS5cumDRpEhITE9G4ceNSb0Lv3r27iiKjd+nhw4fYv38/UlJSUFBQILNt2bJlKoqKqtLGjRsBAM7Ozpg4cSIMDQ1VHBFR1eMcm2pAQ6P8p/pFIhEnDlYDx48fR/fu3eHi4oIbN27A3d0d9+7dgyAIaN68OaKjo1UdIlWhly9fQhAE6XD0/fv3sXfvXri5uSEgIEDF0REpF9exqQYkEkm5P0xqqocpU6ZgwoQJuHr1KvT09LB79248ePAAvr6++Pzzz1UdHlWxoKAgbN68GUDx0g8tWrTA0qVLERQUhIiICBVHR6RcTGyIqoGkpCQMHDgQAKClpYWXL1/CyMgIs2fPxqJFi1QcHVW1S5cuoU2bNgCAXbt2oUaNGrh//z42b96s0LIARB8CzrFRUytWrMBXX30FPT29Cv/h4hom6s/Q0BD5+fkAAHt7e9y5cweNGjUCAGRkZKgyNHoHXrx4AWNjYwDAsWPH0LNnT2hoaKBVq1a4f/++iqMjUi4mNmpq+fLl6NevH/T09LB8+fJy64lEIiY21UCrVq1w9uxZuLm5oUuXLpgwYQKuXLmCPXv2oFWrVqoOj6pY3bp1sW/fPvTo0QNHjx5FaGgoACA9PR0mJiYqjo5IuTh5mKgauHv3LnJzc+Hh4YEXL15g4sSJOHPmDOrWrYvly5fDyclJ1SFSFdq1axf69u0LsViMdu3aISoqCgCwYMECnDp1Cn/++aeKIyRSHiY2RNXA4MGD8eWXX6Jdu3Yyr9Sg6iMtLQ2pqalo0qSJ9EnJc+fOwcTEBA0bNlRxdETKw8SmmuAaJtVb9+7dcezYMVhaWqJPnz7o378/mjZtquqw6B27ffs27ty5g7Zt20JfXx+CIDDRJbXDxKYa4BomBBQ/5vv7779j27ZtOH36NBo0aIAvv/wSffv2hbOzs6rDoyqUmZmJXr164cSJExCJRLh16xZq166NoUOHwszMDEuXLlV1iERKw8e9qwGuYUIAYGZmhq+++goxMTG4f/8+Bg8ejC1btqBu3bqqDo2qWGhoKLS1tZGSkiLzzrjevXvjyJEjKoyMSPmY2FQDXMOE/quwsBAXLlzA33//jXv37sHW1lbVIVEVO3bsGBYtWoRatWrJlNerV4+Pe5PaYWJTDZS1hkkJrmFSfZw4cQLDhw+Hra0tBg4cCGNjYxw4cAAPHjxQdWhUxfLy8mR6akpkZGTIvPGbSB1wHZtqgGuYUK1atZCZmYnAwECsXbsW3bp1g56enqrDonekbdu22Lx5M+bMmQOgeP0qiUSCJUuWwN/fX8XRESkXJw9XA1zDhNatW4fPP/8c5ubmqg6FVCAxMRF+fn7w9PREdHQ0unfvjmvXruHp06c4e/Ys6tSpo+oQiZSGiY2aE4vFOHPmDDw8PPhHjaiaSklJgZaWFtauXYuLFy9CIpGgefPmGDVqFAoLC+Ho6KjqEImUholNNaCnp4ekpCS4uLioOhQiUgFNTU2kpqbCxsZGpjwzMxM2NjYQi8UqioxI+Th5uBpo3Lgx7t69q+owiEhFyvv/19zcXM61IrXDycPVwLx58zBx4kTMmTMHnp6eMDQ0lNnOl+ARqaewsDAAxZOFp0+fLvNklFgsxt9//80VqEntcCiqGih5LwwAmeXTS5ZTZzc0kXoqeeLp5MmT8Pb2ho6OjnSbjo4OnJ2dMXHiRNSrV09VIRIpHXtsqoGNGzfCwcEBmpqaMuUSiQQpKSkqioqIqtqJEycAFL8E9ccff2TvLFUL7LGpBjhxkIiIqgtOHq4GynuDLycOEhGRuuFQlBr778TBadOmceIgERGpPSY2aiw+Ph5AcY/NlStXSk0cbNKkCSZOnKiq8IiIiJSOc2yqAU4cJCKi6oKJDREREakNTh4mIiIitcHEhoiIiNQGExsiIiJSG0xsiIiISG0wsSGid+7evXsQiURISEgAAMTExEAkEuH58+cqjYuIPnxMbIiUYNCgQRCJRBCJRNDW1oatrS06duyIDRs2QCKRKNRWZGQkzMzMqibQNxg0aBA+/fTTCuulp6fj66+/hqOjI3R1dVGjRg0EBgYiLi6u0sf28fFBamoqTE1NAajuOyCiDx8X6CNSkk8++QQbN26EWCzGkydPcOTIEYwbNw67du3C/v37oaWlHv+5BQcHo7CwEJs2bULt2rXx5MkTHD9+HE+fPq10mzo6OqhRo4YSoySiaksgorc2cOBAISgoqFT58ePHBQDCzz//LC1bunSp4O7uLhgYGAi1atUSRowYIeTk5AiCIAgnTpwQAMj8zJgxQxAEQdiyZYvg6ekpGBkZCba2tsIXX3whPHnyRNru06dPhb59+wpWVlaCnp6eULduXWHDhg3S7Q8fPhR69eolmJmZCRYWFkL37t2F5ORkQRAEYcaMGaWOe+LEiVLn8+zZMwGAEBMT88bvA4CwevVq4ZNPPhH09PQEZ2dn4ffff5duT05OFgAI8fHxMuf97NmzN34HREQV4VAUURVq164dmjRpgj179kjLNDQ0sGLFCly9ehWbNm1CdHQ0vvnmGwDFQzLh4eEwMTFBamoqUlNTpa+9KCgowJw5c/DPP/9g3759SE5OxqBBg6TtTps2DYmJifjzzz+RlJSEiIgIWFlZAQBevHgBf39/GBkZ4dSpUzhz5gyMjIzwySefoKCgABMnTkSvXr3wySefSI/r4+NT6nyMjIxgZGSEffv2IT8//43nPm3aNAQHB+Off/7Bl19+iS+++AJJSUkVfmdv+g6IiCqk6syKSB2U12MjCILQu3dvwdXVtdx9f//9d8HS0lL6eePGjYKpqWmFxzx37pwAQNrb061bN2Hw4MFl1l2/fr3QoEEDQSKRSMvy8/MFfX194ejRoxWew3/t2rVLMDc3F/T09AQfHx9hypQpwj///CNTB4AQEhIiU9ayZUthxIgRgiC8ucdGEOT/DoiIXsceG6IqJggCRCKR9POJEyfQsWNH1KxZE8bGxhgwYAAyMzORl5f3xnbi4+MRFBQEJycnGBsbw8/PDwCQkpICABgxYgS2b9+Opk2b4ptvvkFsbKx034sXL+L27dswNjaW9rpYWFjg1atXuHPnjkLnExwcjMePH2P//v0IDAxETEwMmjdvjsjISJl63t7epT7L02NDRPQ2mNgQVbGkpCS4uLgAAO7fv4/OnTvD3d0du3fvxsWLF7Fq1SoAQGFhYblt5OXlISAgAEZGRti6dSvOnz+PvXv3AigeogKATp064f79+xg/fjweP36M9u3bS4dwJBIJPD09kZCQIPNz8+ZN9O3bV+Fz0tPTQ8eOHTF9+nTExsZi0KBBmDFjRoX7/TfBIyKqCkxsiKpQdHQ0rly5guDgYADAhQsXUFRUhKVLl6JVq1aoX78+Hj9+LLOPjo4OxGKxTNn169eRkZGBhQsXok2bNmjYsCHS09NLHc/a2hqDBg3C1q1bER4ejnXr1gEAmjdvjlu3bsHGxgZ169aV+Sl5xLqs48rLzc2tVI/TX3/9Vepzw4YN5WrvbWIhouqNiQ2RkuTn5yMtLQ2PHj3CpUuXMH/+fAQFBaFr164YMGAAAKBOnTooKirCTz/9hLt372LLli1Ys2aNTDvOzs7Izc3F8ePHkZGRgRcvXsDR0RE6OjrS/fbv3485c+bI7Dd9+nT88ccfuH37Nq5du4aDBw/C1dUVANCvXz9YWVkhKCgIp0+fRnJyMk6ePIlx48bh4cOH0uNevnwZN27cQEZGRpk9SJmZmWjXrh22bt2Ky5cvIzk5GTt37sTixYsRFBQkU3fnzp3YsGEDbt68iRkzZuDcuXMYPXq0XN9lWd8BEZFcVD3Jh0gdDBw4UPpospaWlmBtbS106NBB2LBhgyAWi2XqLlu2TLCzsxP09fWFwMBAYfPmzTITZwVBEEJCQgRLS0uZR523bdsmODs7C7q6uoK3t7ewf/9+mQm4c+bMEVxdXQV9fX3BwsJCCAoKEu7evSttMzU1VRgwYIBgZWUl6OrqCrVr1xaGDx8uZGVlCYIgCOnp6ULHjh0FIyOjch/3fvXqlfDtt98KzZs3F0xNTQUDAwOhQYMGwvfffy+8ePFCWg+AsGrVKqFjx46Crq6u4OTkJPz222/S7RVNHi7vOyAiqohIEARBVUkVEaknkUiEvXv3yrWSMRGRMnEoioiIiNQGExsiIiJSG+rx8hoieq9whJuIVIU9NkRERKQ2mNgQERGR2mBiQ0RERGqDiQ0RERGpDSY2REREpDaY2BAREZHaYGJDREREaoOJDREREakNJjZERESkNv4Pek9XiDnqnTYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Visualize class breakdown per split as a stacked bar chart\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# build count table\n",
    "split_counts = pd.DataFrame({\n",
    "    'train': train_df['label'].value_counts(normalize=True),\n",
    "    'val':   val_df  ['label'].value_counts(normalize=True),\n",
    "    'test':  test_df ['label'].value_counts(normalize=True),\n",
    "})\n",
    "\n",
    "# transpose so splits are on the x‐axis\n",
    "split_counts = split_counts.T\n",
    "\n",
    "# choose a simple color palette (you can tweak hex codes as you like)\n",
    "colors = ['#4c72b0', '#dd8452', '#55a868']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "split_counts.plot(\n",
    "    kind='bar',\n",
    "    stacked=True,\n",
    "    color=colors,\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "ax.set_title(\"Class Proportion by Split\")\n",
    "ax.set_xlabel(\"Dataset Split\")\n",
    "ax.set_ylabel(\"Proportion of samples\")\n",
    "ax.legend(title=\"Class\", loc=\"upper right\", bbox_to_anchor=(1.3, 1))\n",
    "\n",
    "# annotate percentages on each bar segment\n",
    "for i, split in enumerate(split_counts.index):\n",
    "    cum = 0\n",
    "    for j, cls in enumerate(split_counts.columns):\n",
    "        val = split_counts.loc[split, cls]\n",
    "        ax.text(\n",
    "            i,                              # x position (bar)\n",
    "            cum + val / 2,                  # y position (middle of segment)\n",
    "            f\"{val:.0%}\",                   # label, e.g. “33%”\n",
    "            ha='center', va='center',\n",
    "            color='white' if val > 0.1 else 'black',\n",
    "            fontsize= 9\n",
    "        )\n",
    "        cum += val\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "# optionally save:\n",
    "# fig.savefig(config['paths']['figures_dir'] + \"split_class_proportions.png\", dpi=150)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine‑tune BERT‑base\n",
    "\n",
    "I start by fine‑tuning `bert-base-uncased` for 3 epochs (per `config.yaml`), using my `CustomTrainer` with weighted cross‑entropy or focal loss as configured.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🖥️  GPU available: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "   • CUDA devices: 1\n",
      "   • Current CUDA device index: 0\n",
      "   • Allocated: 0.00 GB\n",
      "   • Cached:    0.00 GB\n"
     ]
    }
   ],
   "source": [
    "# ── Device Check ────────────────────────────────────────────────────────────\n",
    "import torch  # PyTorch core library\n",
    "\n",
    "# Report whether I have CUDA (GPU) available or are falling back to CPU\n",
    "if torch.cuda.is_available():\n",
    "    print(\"🖥️  GPU available:\", torch.cuda.get_device_name(0))\n",
    "    print(\"   • CUDA devices:\", torch.cuda.device_count())\n",
    "    print(\"   • Current CUDA device index:\", torch.cuda.current_device())\n",
    "    # Optional: memory stats\n",
    "    print(f\"   • Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    print(f\"   • Cached:    {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"🖥️  No GPU detected; using CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ Using preferred GPU → NVIDIA GeForce RTX 4060 Laptop GPU (cuda:0)\n",
      "Data sizes → train: 309520, val: 38690\n",
      "Class weights: [1.0000032308194327, 1.0000032308194327, 0.999993538423763]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\micha\\.conda\\envs\\myenv\\lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU:   7.5%   RAM:  81.0%\n",
      "GPU: NVIDIA GeForce RTX 4060 Laptop GPU | Used: 0.44 GB / 8.59 GB\n",
      "------------------------------------------------------------\n",
      "⏳ Starting trainer.train() …\n",
      "{'loss': 0.6751, 'grad_norm': 12.184558868408203, 'learning_rate': 1.9913845093478074e-05, 'epoch': 0.012923235978288964}\n",
      "{'loss': 0.4652, 'grad_norm': 6.435728549957275, 'learning_rate': 1.982769018695615e-05, 'epoch': 0.025846471956577927}\n",
      "{'loss': 0.4181, 'grad_norm': 6.355397701263428, 'learning_rate': 1.9741535280434225e-05, 'epoch': 0.03876970793486689}\n",
      "{'loss': 0.3739, 'grad_norm': 3.1495249271392822, 'learning_rate': 1.9655380373912297e-05, 'epoch': 0.051692943913155855}\n",
      "{'loss': 0.3562, 'grad_norm': 19.24005699157715, 'learning_rate': 1.956922546739037e-05, 'epoch': 0.06461617989144482}\n",
      "{'loss': 0.3337, 'grad_norm': 3.481600284576416, 'learning_rate': 1.948307056086844e-05, 'epoch': 0.07753941586973379}\n",
      "{'loss': 0.3362, 'grad_norm': 6.34691047668457, 'learning_rate': 1.9396915654346516e-05, 'epoch': 0.09046265184802274}\n",
      "{'loss': 0.3438, 'grad_norm': 6.86762809753418, 'learning_rate': 1.931076074782459e-05, 'epoch': 0.10338588782631171}\n",
      "{'loss': 0.3418, 'grad_norm': 2.707477331161499, 'learning_rate': 1.9224605841302664e-05, 'epoch': 0.11630912380460068}\n",
      "{'loss': 0.3303, 'grad_norm': 6.4294352531433105, 'learning_rate': 1.9138450934780736e-05, 'epoch': 0.12923235978288963}\n",
      "{'loss': 0.3174, 'grad_norm': 4.438650131225586, 'learning_rate': 1.905229602825881e-05, 'epoch': 0.1421555957611786}\n",
      "{'loss': 0.3273, 'grad_norm': 2.4184181690216064, 'learning_rate': 1.8966141121736886e-05, 'epoch': 0.15507883173946757}\n",
      "{'loss': 0.2921, 'grad_norm': 3.8520820140838623, 'learning_rate': 1.887998621521496e-05, 'epoch': 0.16800206771775653}\n",
      "{'loss': 0.3153, 'grad_norm': 7.647615909576416, 'learning_rate': 1.879383130869303e-05, 'epoch': 0.18092530369604548}\n",
      "{'loss': 0.2916, 'grad_norm': 2.3443984985351562, 'learning_rate': 1.8707676402171106e-05, 'epoch': 0.19384853967433446}\n",
      "{'loss': 0.3009, 'grad_norm': 5.050531387329102, 'learning_rate': 1.8621521495649178e-05, 'epoch': 0.20677177565262342}\n",
      "{'loss': 0.2738, 'grad_norm': 5.622109889984131, 'learning_rate': 1.8535366589127253e-05, 'epoch': 0.21969501163091237}\n",
      "{'loss': 0.2838, 'grad_norm': 3.160726547241211, 'learning_rate': 1.8449211682605325e-05, 'epoch': 0.23261824760920136}\n",
      "{'loss': 0.2917, 'grad_norm': 2.3261356353759766, 'learning_rate': 1.8363056776083397e-05, 'epoch': 0.2455414835874903}\n",
      "{'loss': 0.2862, 'grad_norm': 7.044527053833008, 'learning_rate': 1.8276901869561473e-05, 'epoch': 0.25846471956577927}\n",
      "{'loss': 0.2839, 'grad_norm': 6.730479717254639, 'learning_rate': 1.819074696303955e-05, 'epoch': 0.27138795554406825}\n",
      "{'loss': 0.2781, 'grad_norm': 7.370373249053955, 'learning_rate': 1.810459205651762e-05, 'epoch': 0.2843111915223572}\n",
      "{'loss': 0.2876, 'grad_norm': 3.58297061920166, 'learning_rate': 1.8018437149995692e-05, 'epoch': 0.29723442750064616}\n",
      "{'loss': 0.2741, 'grad_norm': 14.694893836975098, 'learning_rate': 1.7932282243473768e-05, 'epoch': 0.31015766347893514}\n",
      "{'loss': 0.2747, 'grad_norm': 10.769925117492676, 'learning_rate': 1.7846127336951843e-05, 'epoch': 0.32308089945722407}\n",
      "{'loss': 0.2816, 'grad_norm': 11.57612133026123, 'learning_rate': 1.7759972430429915e-05, 'epoch': 0.33600413543551305}\n",
      "{'loss': 0.2624, 'grad_norm': 15.075637817382812, 'learning_rate': 1.7673817523907987e-05, 'epoch': 0.34892737141380203}\n",
      "{'loss': 0.2624, 'grad_norm': 6.571269512176514, 'learning_rate': 1.758766261738606e-05, 'epoch': 0.36185060739209096}\n",
      "{'loss': 0.2649, 'grad_norm': 5.204372882843018, 'learning_rate': 1.7501507710864135e-05, 'epoch': 0.37477384337037994}\n",
      "{'loss': 0.2819, 'grad_norm': 11.949258804321289, 'learning_rate': 1.741535280434221e-05, 'epoch': 0.3876970793486689}\n",
      "{'loss': 0.2653, 'grad_norm': 9.08554744720459, 'learning_rate': 1.7329197897820282e-05, 'epoch': 0.40062031532695785}\n",
      "{'loss': 0.28, 'grad_norm': 22.79106903076172, 'learning_rate': 1.7243042991298354e-05, 'epoch': 0.41354355130524684}\n",
      "{'loss': 0.2668, 'grad_norm': 1.7618885040283203, 'learning_rate': 1.715688808477643e-05, 'epoch': 0.4264667872835358}\n",
      "{'loss': 0.2728, 'grad_norm': 9.044376373291016, 'learning_rate': 1.7070733178254505e-05, 'epoch': 0.43939002326182475}\n",
      "{'loss': 0.2715, 'grad_norm': 9.6864013671875, 'learning_rate': 1.6984578271732577e-05, 'epoch': 0.45231325924011373}\n",
      "{'loss': 0.2729, 'grad_norm': 8.380631446838379, 'learning_rate': 1.689842336521065e-05, 'epoch': 0.4652364952184027}\n",
      "{'loss': 0.2738, 'grad_norm': 3.9741954803466797, 'learning_rate': 1.6812268458688725e-05, 'epoch': 0.47815973119669164}\n",
      "{'loss': 0.2815, 'grad_norm': 6.568838119506836, 'learning_rate': 1.6726113552166797e-05, 'epoch': 0.4910829671749806}\n",
      "{'loss': 0.2665, 'grad_norm': 5.6145782470703125, 'learning_rate': 1.6639958645644872e-05, 'epoch': 0.5040062031532696}\n",
      "{'loss': 0.2639, 'grad_norm': 3.7026398181915283, 'learning_rate': 1.6553803739122944e-05, 'epoch': 0.5169294391315585}\n",
      "{'loss': 0.2457, 'grad_norm': 2.0177266597747803, 'learning_rate': 1.6467648832601016e-05, 'epoch': 0.5298526751098475}\n",
      "{'loss': 0.2584, 'grad_norm': 5.81569766998291, 'learning_rate': 1.638149392607909e-05, 'epoch': 0.5427759110881365}\n",
      "{'loss': 0.2553, 'grad_norm': 5.096862316131592, 'learning_rate': 1.6295339019557167e-05, 'epoch': 0.5556991470664254}\n",
      "{'loss': 0.2532, 'grad_norm': 5.301826000213623, 'learning_rate': 1.620918411303524e-05, 'epoch': 0.5686223830447144}\n",
      "{'loss': 0.2546, 'grad_norm': 1.3003100156784058, 'learning_rate': 1.612302920651331e-05, 'epoch': 0.5815456190230034}\n",
      "{'loss': 0.2383, 'grad_norm': 16.550533294677734, 'learning_rate': 1.6036874299991387e-05, 'epoch': 0.5944688550012923}\n",
      "{'loss': 0.2663, 'grad_norm': 3.4082229137420654, 'learning_rate': 1.5950719393469462e-05, 'epoch': 0.6073920909795812}\n",
      "{'loss': 0.2593, 'grad_norm': 12.905584335327148, 'learning_rate': 1.5864564486947534e-05, 'epoch': 0.6203153269578703}\n",
      "{'loss': 0.2634, 'grad_norm': 18.214393615722656, 'learning_rate': 1.5778409580425606e-05, 'epoch': 0.6332385629361592}\n",
      "{'loss': 0.2474, 'grad_norm': 2.364790678024292, 'learning_rate': 1.5692254673903678e-05, 'epoch': 0.6461617989144481}\n",
      "{'loss': 0.2588, 'grad_norm': 5.587676048278809, 'learning_rate': 1.5606099767381754e-05, 'epoch': 0.6590850348927372}\n",
      "{'loss': 0.2648, 'grad_norm': 0.8807097673416138, 'learning_rate': 1.551994486085983e-05, 'epoch': 0.6720082708710261}\n",
      "{'loss': 0.2415, 'grad_norm': 9.947172164916992, 'learning_rate': 1.54337899543379e-05, 'epoch': 0.684931506849315}\n",
      "{'loss': 0.2436, 'grad_norm': 0.34054800868034363, 'learning_rate': 1.5347635047815973e-05, 'epoch': 0.6978547428276041}\n",
      "{'loss': 0.2525, 'grad_norm': 4.34541130065918, 'learning_rate': 1.526148014129405e-05, 'epoch': 0.710777978805893}\n",
      "{'loss': 0.2475, 'grad_norm': 3.3268496990203857, 'learning_rate': 1.5175325234772122e-05, 'epoch': 0.7237012147841819}\n",
      "{'loss': 0.2516, 'grad_norm': 5.398063659667969, 'learning_rate': 1.5089170328250196e-05, 'epoch': 0.736624450762471}\n",
      "{'loss': 0.2522, 'grad_norm': 0.8073326945304871, 'learning_rate': 1.5003015421728268e-05, 'epoch': 0.7495476867407599}\n",
      "{'loss': 0.246, 'grad_norm': 6.788999080657959, 'learning_rate': 1.4916860515206342e-05, 'epoch': 0.7624709227190488}\n",
      "{'loss': 0.2443, 'grad_norm': 6.851729869842529, 'learning_rate': 1.4830705608684415e-05, 'epoch': 0.7753941586973379}\n",
      "{'loss': 0.2495, 'grad_norm': 1.0630712509155273, 'learning_rate': 1.474455070216249e-05, 'epoch': 0.7883173946756268}\n",
      "{'loss': 0.2399, 'grad_norm': 6.460484027862549, 'learning_rate': 1.4658395795640563e-05, 'epoch': 0.8012406306539157}\n",
      "{'loss': 0.2572, 'grad_norm': 6.908089637756348, 'learning_rate': 1.4572240889118637e-05, 'epoch': 0.8141638666322047}\n",
      "{'loss': 0.2475, 'grad_norm': 3.826740026473999, 'learning_rate': 1.4486085982596709e-05, 'epoch': 0.8270871026104937}\n",
      "{'loss': 0.242, 'grad_norm': 2.8912975788116455, 'learning_rate': 1.4399931076074784e-05, 'epoch': 0.8400103385887826}\n",
      "{'loss': 0.2558, 'grad_norm': 8.49909496307373, 'learning_rate': 1.4313776169552858e-05, 'epoch': 0.8529335745670716}\n",
      "{'loss': 0.2237, 'grad_norm': 16.771385192871094, 'learning_rate': 1.422762126303093e-05, 'epoch': 0.8658568105453606}\n",
      "{'loss': 0.2477, 'grad_norm': 4.638957500457764, 'learning_rate': 1.4141466356509004e-05, 'epoch': 0.8787800465236495}\n",
      "{'loss': 0.2634, 'grad_norm': 6.05959415435791, 'learning_rate': 1.4055311449987077e-05, 'epoch': 0.8917032825019385}\n",
      "{'loss': 0.2411, 'grad_norm': 4.3286213874816895, 'learning_rate': 1.3969156543465153e-05, 'epoch': 0.9046265184802275}\n",
      "{'loss': 0.2546, 'grad_norm': 9.248021125793457, 'learning_rate': 1.3883001636943225e-05, 'epoch': 0.9175497544585164}\n",
      "{'loss': 0.2338, 'grad_norm': 4.111486434936523, 'learning_rate': 1.3796846730421299e-05, 'epoch': 0.9304729904368054}\n",
      "{'loss': 0.2519, 'grad_norm': 7.932967185974121, 'learning_rate': 1.371069182389937e-05, 'epoch': 0.9433962264150944}\n",
      "{'loss': 0.2412, 'grad_norm': 10.131109237670898, 'learning_rate': 1.3624536917377446e-05, 'epoch': 0.9563194623933833}\n",
      "{'loss': 0.22, 'grad_norm': 8.142133712768555, 'learning_rate': 1.353838201085552e-05, 'epoch': 0.9692426983716723}\n",
      "{'loss': 0.23, 'grad_norm': 17.675323486328125, 'learning_rate': 1.3452227104333594e-05, 'epoch': 0.9821659343499612}\n",
      "{'loss': 0.2536, 'grad_norm': 0.36218389868736267, 'learning_rate': 1.3366072197811666e-05, 'epoch': 0.9950891703282502}\n",
      "{'eval_loss': 0.22874490916728973, 'eval_accuracy': 0.9154820367019901, 'eval_f1': 0.9150964052591188, 'eval_runtime': 506.5753, 'eval_samples_per_second': 76.376, 'eval_steps_per_second': 4.775, 'epoch': 1.0}\n",
      "{'loss': 0.2214, 'grad_norm': 5.4186577796936035, 'learning_rate': 1.327991729128974e-05, 'epoch': 1.0080124063065392}\n",
      "{'loss': 0.2034, 'grad_norm': 3.746562957763672, 'learning_rate': 1.3193762384767815e-05, 'epoch': 1.0209356422848281}\n",
      "{'loss': 0.205, 'grad_norm': 5.404241561889648, 'learning_rate': 1.3107607478245887e-05, 'epoch': 1.033858878263117}\n",
      "{'loss': 0.2012, 'grad_norm': 6.232556343078613, 'learning_rate': 1.302145257172396e-05, 'epoch': 1.046782114241406}\n",
      "{'loss': 0.2025, 'grad_norm': 12.950078010559082, 'learning_rate': 1.2935297665202034e-05, 'epoch': 1.059705350219695}\n",
      "{'loss': 0.2154, 'grad_norm': 3.8914246559143066, 'learning_rate': 1.2849142758680108e-05, 'epoch': 1.072628586197984}\n",
      "{'loss': 0.2009, 'grad_norm': 11.455574989318848, 'learning_rate': 1.2762987852158182e-05, 'epoch': 1.085551822176273}\n",
      "{'loss': 0.2036, 'grad_norm': 5.282279968261719, 'learning_rate': 1.2676832945636255e-05, 'epoch': 1.098475058154562}\n",
      "{'loss': 0.2043, 'grad_norm': 0.7313770651817322, 'learning_rate': 1.2590678039114327e-05, 'epoch': 1.1113982941328509}\n",
      "{'loss': 0.1987, 'grad_norm': 15.137126922607422, 'learning_rate': 1.2504523132592403e-05, 'epoch': 1.1243215301111398}\n",
      "{'loss': 0.1908, 'grad_norm': 14.032855987548828, 'learning_rate': 1.2418368226070477e-05, 'epoch': 1.1372447660894287}\n",
      "{'loss': 0.2083, 'grad_norm': 2.317748546600342, 'learning_rate': 1.2332213319548549e-05, 'epoch': 1.1501680020677179}\n",
      "{'loss': 0.1823, 'grad_norm': 6.812184810638428, 'learning_rate': 1.2246058413026622e-05, 'epoch': 1.1630912380460068}\n",
      "{'loss': 0.1979, 'grad_norm': 0.9930015206336975, 'learning_rate': 1.2159903506504696e-05, 'epoch': 1.1760144740242957}\n",
      "{'loss': 0.2032, 'grad_norm': 8.834428787231445, 'learning_rate': 1.2073748599982772e-05, 'epoch': 1.1889377100025846}\n",
      "{'loss': 0.1883, 'grad_norm': 6.491714954376221, 'learning_rate': 1.1987593693460844e-05, 'epoch': 1.2018609459808736}\n",
      "{'loss': 0.1956, 'grad_norm': 1.417237401008606, 'learning_rate': 1.1901438786938917e-05, 'epoch': 1.2147841819591625}\n",
      "{'loss': 0.179, 'grad_norm': 9.94859504699707, 'learning_rate': 1.181528388041699e-05, 'epoch': 1.2277074179374514}\n",
      "{'loss': 0.1897, 'grad_norm': 12.435208320617676, 'learning_rate': 1.1729128973895065e-05, 'epoch': 1.2406306539157406}\n",
      "{'loss': 0.2078, 'grad_norm': 13.209046363830566, 'learning_rate': 1.1642974067373139e-05, 'epoch': 1.2535538898940295}\n",
      "{'loss': 0.1914, 'grad_norm': 10.49775218963623, 'learning_rate': 1.1556819160851212e-05, 'epoch': 1.2664771258723184}\n",
      "{'loss': 0.1968, 'grad_norm': 9.03569221496582, 'learning_rate': 1.1470664254329284e-05, 'epoch': 1.2794003618506073}\n",
      "{'loss': 0.2021, 'grad_norm': 7.640361785888672, 'learning_rate': 1.1384509347807358e-05, 'epoch': 1.2923235978288963}\n",
      "{'loss': 0.2004, 'grad_norm': 3.7026591300964355, 'learning_rate': 1.1298354441285433e-05, 'epoch': 1.3052468338071854}\n",
      "{'loss': 0.1983, 'grad_norm': 8.489011764526367, 'learning_rate': 1.1212199534763506e-05, 'epoch': 1.3181700697854744}\n",
      "{'loss': 0.1901, 'grad_norm': 0.5606158375740051, 'learning_rate': 1.112604462824158e-05, 'epoch': 1.3310933057637633}\n",
      "{'loss': 0.1864, 'grad_norm': 14.680021286010742, 'learning_rate': 1.1039889721719651e-05, 'epoch': 1.3440165417420522}\n",
      "{'loss': 0.1975, 'grad_norm': 10.9469575881958, 'learning_rate': 1.0953734815197727e-05, 'epoch': 1.3569397777203411}\n",
      "{'loss': 0.2105, 'grad_norm': 8.591601371765137, 'learning_rate': 1.08675799086758e-05, 'epoch': 1.36986301369863}\n",
      "{'loss': 0.1965, 'grad_norm': 4.6556396484375, 'learning_rate': 1.0781425002153874e-05, 'epoch': 1.382786249676919}\n",
      "{'loss': 0.2117, 'grad_norm': 13.646517753601074, 'learning_rate': 1.0695270095631946e-05, 'epoch': 1.3957094856552081}\n",
      "{'loss': 0.1941, 'grad_norm': 6.219468116760254, 'learning_rate': 1.060911518911002e-05, 'epoch': 1.408632721633497}\n",
      "{'loss': 0.2028, 'grad_norm': 0.3638759255409241, 'learning_rate': 1.0522960282588095e-05, 'epoch': 1.421555957611786}\n",
      "{'loss': 0.2042, 'grad_norm': 12.771842956542969, 'learning_rate': 1.0436805376066167e-05, 'epoch': 1.434479193590075}\n",
      "{'loss': 0.1927, 'grad_norm': 7.298154830932617, 'learning_rate': 1.0350650469544241e-05, 'epoch': 1.4474024295683638}\n",
      "{'loss': 0.188, 'grad_norm': 9.499245643615723, 'learning_rate': 1.0264495563022315e-05, 'epoch': 1.460325665546653}\n",
      "{'loss': 0.2008, 'grad_norm': 5.216103553771973, 'learning_rate': 1.017834065650039e-05, 'epoch': 1.473248901524942}\n",
      "{'loss': 0.2086, 'grad_norm': 3.708038330078125, 'learning_rate': 1.0092185749978462e-05, 'epoch': 1.4861721375032309}\n",
      "{'loss': 0.1832, 'grad_norm': 0.3616108298301697, 'learning_rate': 1.0006030843456536e-05, 'epoch': 1.4990953734815198}\n",
      "{'loss': 0.2, 'grad_norm': 0.8338373899459839, 'learning_rate': 9.919875936934608e-06, 'epoch': 1.5120186094598087}\n",
      "{'loss': 0.1965, 'grad_norm': 8.540262222290039, 'learning_rate': 9.833721030412684e-06, 'epoch': 1.5249418454380979}\n",
      "{'loss': 0.1883, 'grad_norm': 7.280529022216797, 'learning_rate': 9.747566123890756e-06, 'epoch': 1.5378650814163866}\n",
      "{'loss': 0.2051, 'grad_norm': 9.97177791595459, 'learning_rate': 9.66141121736883e-06, 'epoch': 1.5507883173946757}\n",
      "{'loss': 0.2, 'grad_norm': 1.3662587404251099, 'learning_rate': 9.575256310846903e-06, 'epoch': 1.5637115533729646}\n",
      "{'loss': 0.1986, 'grad_norm': 22.95154571533203, 'learning_rate': 9.489101404324977e-06, 'epoch': 1.5766347893512536}\n",
      "{'loss': 0.2024, 'grad_norm': 0.4995168447494507, 'learning_rate': 9.40294649780305e-06, 'epoch': 1.5895580253295425}\n",
      "{'loss': 0.2024, 'grad_norm': 0.8000299334526062, 'learning_rate': 9.316791591281124e-06, 'epoch': 1.6024812613078314}\n",
      "{'loss': 0.2104, 'grad_norm': 11.631255149841309, 'learning_rate': 9.230636684759198e-06, 'epoch': 1.6154044972861206}\n",
      "{'loss': 0.1938, 'grad_norm': 2.1221401691436768, 'learning_rate': 9.144481778237272e-06, 'epoch': 1.6283277332644093}\n",
      "{'loss': 0.1884, 'grad_norm': 0.22844503819942474, 'learning_rate': 9.058326871715345e-06, 'epoch': 1.6412509692426984}\n",
      "{'loss': 0.2081, 'grad_norm': 4.195463180541992, 'learning_rate': 8.972171965193418e-06, 'epoch': 1.6541742052209873}\n",
      "{'loss': 0.1911, 'grad_norm': 5.268805980682373, 'learning_rate': 8.886017058671493e-06, 'epoch': 1.6670974411992763}\n",
      "{'loss': 0.1987, 'grad_norm': 3.8077616691589355, 'learning_rate': 8.799862152149565e-06, 'epoch': 1.6800206771775654}\n",
      "{'loss': 0.2055, 'grad_norm': 2.2626893520355225, 'learning_rate': 8.713707245627639e-06, 'epoch': 1.6929439131558541}\n",
      "{'loss': 0.2032, 'grad_norm': 0.22753332555294037, 'learning_rate': 8.627552339105712e-06, 'epoch': 1.7058671491341433}\n",
      "{'loss': 0.1992, 'grad_norm': 0.47050920128822327, 'learning_rate': 8.541397432583786e-06, 'epoch': 1.7187903851124322}\n",
      "{'loss': 0.1905, 'grad_norm': 11.285271644592285, 'learning_rate': 8.45524252606186e-06, 'epoch': 1.7317136210907211}\n",
      "{'loss': 0.187, 'grad_norm': 4.512368679046631, 'learning_rate': 8.369087619539934e-06, 'epoch': 1.74463685706901}\n",
      "{'loss': 0.196, 'grad_norm': 0.8527282476425171, 'learning_rate': 8.282932713018007e-06, 'epoch': 1.757560093047299}\n",
      "{'loss': 0.2055, 'grad_norm': 3.015063524246216, 'learning_rate': 8.19677780649608e-06, 'epoch': 1.7704833290255881}\n",
      "{'loss': 0.1881, 'grad_norm': 2.1039979457855225, 'learning_rate': 8.110622899974155e-06, 'epoch': 1.7834065650038768}\n",
      "{'loss': 0.1851, 'grad_norm': 7.6821184158325195, 'learning_rate': 8.024467993452227e-06, 'epoch': 1.796329800982166}\n",
      "{'loss': 0.1859, 'grad_norm': 4.8128342628479, 'learning_rate': 7.938313086930302e-06, 'epoch': 1.809253036960455}\n",
      "{'loss': 0.1835, 'grad_norm': 26.35439109802246, 'learning_rate': 7.852158180408374e-06, 'epoch': 1.8221762729387438}\n",
      "{'loss': 0.1889, 'grad_norm': 1.0099083185195923, 'learning_rate': 7.766003273886448e-06, 'epoch': 1.8350995089170328}\n",
      "{'loss': 0.209, 'grad_norm': 6.777827739715576, 'learning_rate': 7.679848367364522e-06, 'epoch': 1.8480227448953217}\n",
      "{'loss': 0.2012, 'grad_norm': 9.463371276855469, 'learning_rate': 7.593693460842596e-06, 'epoch': 1.8609459808736108}\n",
      "{'loss': 0.2037, 'grad_norm': 11.904932022094727, 'learning_rate': 7.507538554320669e-06, 'epoch': 1.8738692168518996}\n",
      "{'loss': 0.1998, 'grad_norm': 7.217942714691162, 'learning_rate': 7.421383647798742e-06, 'epoch': 1.8867924528301887}\n",
      "{'loss': 0.193, 'grad_norm': 6.748416423797607, 'learning_rate': 7.335228741276817e-06, 'epoch': 1.8997156888084776}\n",
      "{'loss': 0.1822, 'grad_norm': 5.354523181915283, 'learning_rate': 7.24907383475489e-06, 'epoch': 1.9126389247867666}\n",
      "{'loss': 0.1835, 'grad_norm': 0.1318240463733673, 'learning_rate': 7.162918928232963e-06, 'epoch': 1.9255621607650557}\n",
      "{'loss': 0.1725, 'grad_norm': 0.7374141812324524, 'learning_rate': 7.076764021711037e-06, 'epoch': 1.9384853967433444}\n",
      "{'loss': 0.1877, 'grad_norm': 10.063443183898926, 'learning_rate': 6.990609115189111e-06, 'epoch': 1.9514086327216336}\n",
      "{'loss': 0.1938, 'grad_norm': 13.740924835205078, 'learning_rate': 6.904454208667184e-06, 'epoch': 1.9643318686999225}\n",
      "{'loss': 0.1845, 'grad_norm': 10.446634292602539, 'learning_rate': 6.818299302145258e-06, 'epoch': 1.9772551046782114}\n",
      "{'loss': 0.2039, 'grad_norm': 11.135940551757812, 'learning_rate': 6.732144395623331e-06, 'epoch': 1.9901783406565003}\n",
      "{'eval_loss': 0.30707886815071106, 'eval_accuracy': 0.8967174980615146, 'eval_f1': 0.8963955742281492, 'eval_runtime': 516.6225, 'eval_samples_per_second': 74.89, 'eval_steps_per_second': 4.682, 'epoch': 2.0}\n",
      "{'train_runtime': 28904.4685, 'train_samples_per_second': 32.125, 'train_steps_per_second': 2.008, 'train_loss': 0.23879013603804, 'epoch': 2.0}\n",
      "✅ Finished in 481.78 min\n",
      "CPU:   7.8%   RAM:  82.0%\n",
      "GPU: NVIDIA GeForce RTX 4060 Laptop GPU | Used: 1.33 GB / 8.59 GB\n",
      "------------------------------------------------------------\n",
      "{'eval_loss': 0.22874490916728973, 'eval_accuracy': 0.9154820367019901, 'eval_f1': 0.9150964052591188, 'eval_runtime': 515.2243, 'eval_samples_per_second': 75.094, 'eval_steps_per_second': 4.695, 'epoch': 2.0}\n",
      "📝 Validation metrics: {'eval_loss': 0.22874490916728973, 'eval_accuracy': 0.9154820367019901, 'eval_f1': 0.9150964052591188, 'eval_runtime': 515.2243, 'eval_samples_per_second': 75.094, 'eval_steps_per_second': 4.695, 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": [
    "# ── Full BERT Fine‑Tuning Pipeline (Prefer RTX 4060) ─────────────────────────\n",
    "\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import yaml                          # for reading config.yaml\n",
    "import psutil                        # for CPU & RAM usage\n",
    "import pandas as pd                 # DataFrame I/O\n",
    "import numpy as np                  # numeric ops\n",
    "import torch                        # PyTorch core\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    "    logging as hf_logging\n",
    ")\n",
    "from utils import model_utils       # your wrappers around HF tokenizers/models/metrics\n",
    "\n",
    "# ─ Silence all non‑error HF logs ─────────────────────────────────────────────\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "def show_resources():\n",
    "    \"\"\"Print current CPU%, RAM%, and (if GPU) GPU memory usage.\"\"\"\n",
    "    cpu_pct = psutil.cpu_percent(interval=0.5)\n",
    "    ram_pct = psutil.virtual_memory().percent\n",
    "    print(f\"CPU: {cpu_pct:5.1f}%   RAM: {ram_pct:5.1f}%\")\n",
    "    if torch.cuda.is_available():\n",
    "        props    = torch.cuda.get_device_properties(0)\n",
    "        total_gb = props.total_memory / 1e9\n",
    "        used_gb  = torch.cuda.memory_allocated(0) / 1e9\n",
    "        print(f\"GPU: {props.name} | Used: {used_gb:.2f} GB / {total_gb:.2f} GB\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# ── 1️⃣ Device selection: prefer an RTX 4060, else any CUDA GPU, else CPU ───\n",
    "if torch.cuda.is_available():\n",
    "    devices = [(i, torch.cuda.get_device_name(i)) for i in range(torch.cuda.device_count())]\n",
    "    # look for “RTX 4060” in any device name:\n",
    "    idx_4060 = next((i for i, name in devices if \"RTX 4060\" in name), None)\n",
    "    if idx_4060 is not None:\n",
    "        device = torch.device(f\"cuda:{idx_4060}\")\n",
    "        print(f\"✔️ Using preferred GPU → {devices[idx_4060][1]} (cuda:{idx_4060})\")\n",
    "    else:\n",
    "        # no 4060 found, pick the first GPU\n",
    "        idx0, name0 = devices[0]\n",
    "        device = torch.device(f\"cuda:{idx0}\")\n",
    "        print(f\"ℹ️ RTX 4060 not found, using GPU → {name0} (cuda:{idx0})\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"⚠️ No CUDA GPU detected — falling back to CPU\")\n",
    "\n",
    "# ── 2️⃣ Load config & data splits ─────────────────────────────────────────────\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "train_df = pd.read_parquet(config['paths']['train_data'])\n",
    "val_df   = pd.read_parquet(config['paths']['val_data'])\n",
    "print(f\"Data sizes → train: {len(train_df)}, val: {len(val_df)}\")\n",
    "\n",
    "# ── 3️⃣ Compute class weights (for weighted CE / focal loss) ────────────────\n",
    "labels, counts = np.unique(train_df['label'], return_counts=True)\n",
    "inv_freq       = (1.0 / counts) * np.mean(counts)\n",
    "weight_list    = [0.0] * len(inv_freq)\n",
    "for lab, w in zip(labels, inv_freq):\n",
    "    idx = config['model']['label_mapping'][lab]\n",
    "    weight_list[idx] = float(w)\n",
    "print(\"Class weights:\", weight_list)\n",
    "\n",
    "# ── 4️⃣ Initialize tokenizer & model, send to device ────────────────────────\n",
    "tokenizer = model_utils.get_tokenizer(\"bert\")\n",
    "model     = model_utils.get_model(\"bert\", num_labels=3).to(device)\n",
    "\n",
    "# ── 5️⃣ Tokenize all texts (pad+truncate to max_length) ──────────────────────\n",
    "max_len   = config['training']['max_length']['bert_roberta']\n",
    "train_enc = tokenizer(\n",
    "    train_df['text'].tolist(),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=max_len\n",
    ")\n",
    "val_enc   = tokenizer(\n",
    "    val_df['text'].tolist(),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=max_len\n",
    ")\n",
    "\n",
    "# ── 6️⃣ Wrap tokenized data in a minimal Dataset (CPU side) ─────────────────\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels    = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k,v in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "train_labels   = [config['model']['label_mapping'][l] for l in train_df['label']]\n",
    "val_labels     = [config['model']['label_mapping'][l] for l in val_df['label']]\n",
    "train_dataset  = TextDataset(train_enc, train_labels)\n",
    "val_dataset    = TextDataset(val_enc,   val_labels)\n",
    "\n",
    "# ── 7️⃣ Configure TrainingArguments ─────────────────────────────────────────\n",
    "training_args = TrainingArguments(\n",
    "    output_dir                  = config['paths']['model_dirs']['bert'],\n",
    "    num_train_epochs            = int(config['training']['epochs']['bert']),\n",
    "    per_device_train_batch_size = int(config['training']['batch_size']['bert']),\n",
    "    per_device_eval_batch_size  = int(config['training']['batch_size']['bert']),\n",
    "    learning_rate               = float(config['training']['learning_rate']),\n",
    "\n",
    "    evaluation_strategy         = \"epoch\",\n",
    "    save_strategy               = \"epoch\",\n",
    "    load_best_model_at_end      = True,\n",
    "    metric_for_best_model       = \"f1\",\n",
    "\n",
    "    logging_strategy            = \"steps\",\n",
    "    logging_steps               = 250,\n",
    "    report_to                   = \"none\",\n",
    "    dataloader_pin_memory       = False   # avoid pin_memory errors\n",
    ")\n",
    "\n",
    "# ── 8️⃣ Instantiate CustomTrainer w/ early stopping & focal/weighted loss ──\n",
    "trainer = model_utils.CustomTrainer(\n",
    "    model           = model,\n",
    "    args            = training_args,\n",
    "    train_dataset   = train_dataset,\n",
    "    eval_dataset    = val_dataset,\n",
    "    compute_metrics = model_utils.compute_metrics,\n",
    "    callbacks       = [\n",
    "        EarlyStoppingCallback(\n",
    "            early_stopping_patience=int(config['training']['early_stopping_patience'])\n",
    "        )\n",
    "    ],\n",
    "    use_focal       = bool(config['training']['use_focal_loss']),\n",
    "    alpha           = weight_list\n",
    ")\n",
    "\n",
    "# ── 9️⃣ Run training, with resource printouts & elapsed time ────────────────\n",
    "show_resources()\n",
    "t0 = time.time()\n",
    "print(\"⏳ Starting trainer.train() …\")\n",
    "trainer.train()\n",
    "t1 = time.time()\n",
    "print(f\"✅ Finished in {(t1 - t0)/60:.2f} min\")\n",
    "show_resources()\n",
    "\n",
    "# 🔟 Final evaluation & logging ───────────────────────────────────────────────\n",
    "metrics = trainer.evaluate()\n",
    "print(\"📝 Validation metrics:\", metrics)\n",
    "logging.info(f\"BERT validation metrics → {metrics}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Validation Results\n",
    "\n",
    "After fine‑tuning **BERT‑base**, I observe:\n",
    "\n",
    "- **Loss:** *`<insert bert_metrics['eval_loss'] here>`*  \n",
    "- **Accuracy:** *`<insert bert_metrics['eval_accuracy'] here>`*  \n",
    "- **Macro‑F1:** *`<insert bert_metrics['eval_f1'] here>`*\n",
    "\n",
    "These results provide our baseline before moving on to RoBERTa. I’ll compare these metrics directly in the next section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine‑tune RoBERTa‑base\n",
    "\n",
    "Now I train `roberta-base` under the same conditions to see if it improves over BERT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 1) Grab the BERT logging interval so we can reuse it ──────────────────────\n",
    "# (this cell should be **after** my BERT setup cell where you defined `training_args`)\n",
    "logging_steps = training_args.logging_steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 2) Prepare RoBERTa tokenizer & model ───────────────────────────────────\n",
    "from transformers import TrainingArguments, EarlyStoppingCallback\n",
    "from torch.utils.data import Dataset\n",
    "import time, logging\n",
    "\n",
    "# 2.1) Tokenizer + model (+ to same `device` you detected earlier)\n",
    "tokenizer_roberta = model_utils.get_tokenizer(\"roberta\")\n",
    "model_roberta     = model_utils.get_model(\"roberta\", num_labels=3).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 3) (Re-)tokenize texts for RoBERTa ─────────────────────────────────────\n",
    "train_enc_roberta = tokenizer_roberta(\n",
    "    train_df['text'].tolist(),\n",
    "    padding=True, truncation=True,\n",
    "    max_length=config['training']['max_length']['bert_roberta']\n",
    ")\n",
    "val_enc_roberta = tokenizer_roberta(\n",
    "    val_df['text'].tolist(),\n",
    "    padding=True, truncation=True,\n",
    "    max_length=config['training']['max_length']['bert_roberta']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 4) Wrap into torch Datasets (reuse mine TextDataset class) ─────────────\n",
    "train_dataset_roberta = TextDataset(\n",
    "    train_enc_roberta,\n",
    "    [config['model']['label_mapping'][l] for l in train_df['label']]\n",
    ")\n",
    "val_dataset_roberta   = TextDataset(\n",
    "    val_enc_roberta,\n",
    "    [config['model']['label_mapping'][l] for l in val_df['label']]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 5) Define TrainingArguments for RoBERTa ─────────────────────────────────\n",
    "training_args_roberta = TrainingArguments(\n",
    "    output_dir                  = config['paths']['model_dirs']['roberta'],\n",
    "    num_train_epochs            = int(config['training']['epochs']['roberta']),\n",
    "    per_device_train_batch_size = int(config['training']['batch_size']['roberta']),\n",
    "    per_device_eval_batch_size  = int(config['training']['batch_size']['roberta']),\n",
    "    learning_rate               = float(config['training']['learning_rate']),\n",
    "\n",
    "    evaluation_strategy   = \"epoch\",\n",
    "    save_strategy         = \"epoch\",\n",
    "    load_best_model_at_end= True,\n",
    "    metric_for_best_model = \"f1\",\n",
    "\n",
    "    logging_strategy      = \"steps\",\n",
    "    logging_steps         = logging_steps,   # ← reuse from BERT cell\n",
    "    report_to             = \"none\",\n",
    "    dataloader_pin_memory = False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99bc38e096134a258ea64a6ac184ceb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\micha\\.conda\\envs\\myenv\\lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\micha\\.cache\\huggingface\\hub\\models--roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b2ed2f1fa5e4a3d8f93aaa5e95f0d0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "297b7bfd4cd74e4c871855ee378049e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "291ef7adad4e46ec912163de604526df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15485a6434a3485a8778ac22437c8701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "600d006eab41400ea1f5a755ae294e18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'logging_steps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 39\u001b[0m\n\u001b[0;32m     23\u001b[0m val_dataset_roberta   \u001b[38;5;241m=\u001b[39m TextDataset(val_enc_roberta,\n\u001b[0;32m     24\u001b[0m                                     [config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_mapping\u001b[39m\u001b[38;5;124m'\u001b[39m][l] \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m val_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# ── 4) Update & reuse TrainingArguments ────────────────────────────\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m#    (point at new output dir, adjust epochs)\u001b[39;00m\n\u001b[0;32m     28\u001b[0m training_args_roberta \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m     29\u001b[0m     output_dir                  \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpaths\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_dirs\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroberta\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     30\u001b[0m     num_train_epochs            \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroberta\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[0;32m     31\u001b[0m     per_device_train_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroberta\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[0;32m     32\u001b[0m     per_device_eval_batch_size  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroberta\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[0;32m     33\u001b[0m     learning_rate               \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[0;32m     34\u001b[0m     evaluation_strategy         \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     35\u001b[0m     save_strategy               \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     36\u001b[0m     load_best_model_at_end      \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     37\u001b[0m     metric_for_best_model       \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     38\u001b[0m     logging_strategy            \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m---> 39\u001b[0m     logging_steps               \u001b[38;5;241m=\u001b[39m \u001b[43mlogging_steps\u001b[49m,      \u001b[38;5;66;03m# reuse the same logging interval\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     report_to                   \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     41\u001b[0m     dataloader_pin_memory       \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     42\u001b[0m )\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# ── 5) Instantiate CustomTrainer for RoBERTa ────────────────────────\u001b[39;00m\n\u001b[0;32m     45\u001b[0m trainer_roberta \u001b[38;5;241m=\u001b[39m model_utils\u001b[38;5;241m.\u001b[39mCustomTrainer(\n\u001b[0;32m     46\u001b[0m     model           \u001b[38;5;241m=\u001b[39m model_roberta,\n\u001b[0;32m     47\u001b[0m     args            \u001b[38;5;241m=\u001b[39m training_args_roberta,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m     alpha           \u001b[38;5;241m=\u001b[39m weight_list\n\u001b[0;32m     56\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'logging_steps' is not defined"
     ]
    }
   ],
   "source": [
    "# ── 1) Prepare RoBERTa tokenizer & model ───────────────────────────\n",
    "tokenizer_roberta = model_utils.get_tokenizer(\"roberta\")\n",
    "model_roberta     = model_utils.get_model(\"roberta\", num_labels=3).to(device)\n",
    "\n",
    "# ── 2) (Re‑)tokenize texts for RoBERTa ─────────────────────────────\n",
    "#    Note: must redo tokenization because RoBERTa's vocab & special tokens differ\n",
    "train_enc_roberta = tokenizer_roberta(\n",
    "    train_df['text'].tolist(),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=config['training']['max_length']['bert_roberta']\n",
    ")\n",
    "val_enc_roberta = tokenizer_roberta(\n",
    "    val_df['text'].tolist(),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=config['training']['max_length']['bert_roberta']\n",
    ")\n",
    "\n",
    "# ── 3) Wrap into torch Datasets ────────────────────────────────────\n",
    "train_dataset_roberta = TextDataset(train_enc_roberta,\n",
    "                                    [config['model']['label_mapping'][l] for l in train_df['label']])\n",
    "val_dataset_roberta   = TextDataset(val_enc_roberta,\n",
    "                                    [config['model']['label_mapping'][l] for l in val_df['label']])\n",
    "\n",
    "# ── 4) Update & reuse TrainingArguments ────────────────────────────\n",
    "#    (point at new output dir, adjust epochs)\n",
    "training_args_roberta = TrainingArguments(\n",
    "    output_dir                  = config['paths']['model_dirs']['roberta'],\n",
    "    num_train_epochs            = int(config['training']['epochs']['roberta']),\n",
    "    per_device_train_batch_size = int(config['training']['batch_size']['roberta']),\n",
    "    per_device_eval_batch_size  = int(config['training']['batch_size']['roberta']),\n",
    "    learning_rate               = float(config['training']['learning_rate']),\n",
    "    evaluation_strategy         = \"epoch\",\n",
    "    save_strategy               = \"epoch\",\n",
    "    load_best_model_at_end      = True,\n",
    "    metric_for_best_model       = \"f1\",\n",
    "    logging_strategy            = \"steps\",\n",
    "    logging_steps               = logging_steps,      # reuse the same logging interval\n",
    "    report_to                   = \"none\",\n",
    "    dataloader_pin_memory       = False\n",
    ")\n",
    "\n",
    "# ── 5) Instantiate CustomTrainer for RoBERTa ────────────────────────\n",
    "trainer_roberta = model_utils.CustomTrainer(\n",
    "    model           = model_roberta,\n",
    "    args            = training_args_roberta,\n",
    "    train_dataset   = train_dataset_roberta,\n",
    "    eval_dataset    = val_dataset_roberta,\n",
    "    compute_metrics = model_utils.compute_metrics,\n",
    "    callbacks       = [EarlyStoppingCallback(\n",
    "                         early_stopping_patience=int(config['training']['early_stopping_patience'])\n",
    "                       )],\n",
    "    use_focal       = bool(config['training']['use_focal_loss']),\n",
    "    alpha           = weight_list\n",
    ")\n",
    "\n",
    "# ── 6) Train & evaluate ────────────────────────────────────────────\n",
    "print(\"⏳ Starting RoBERTa training…\")\n",
    "start = time.time()\n",
    "trainer_roberta.train()\n",
    "elapsed = (time.time() - start) / 60\n",
    "print(f\"✅ RoBERTa training finished in {elapsed:.1f} min\")\n",
    "\n",
    "print(\"📝 Evaluating RoBERTa on validation split…\")\n",
    "roberta_metrics = trainer_roberta.evaluate()\n",
    "print(\"RoBERTa Validation metrics:\", roberta_metrics)\n",
    "logging.info(f\"RoBERTa validation metrics → {roberta_metrics}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "I compare BERT vs RoBERTa on the validation F1 score and choose the best for final deployment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare F1 scores and pick best\n",
    "best_model = \"RoBERTa\" if roberta_metrics['eval_f1'] >= metrics['eval_f1'] else \"BERT\"\n",
    "print(f\"Selected best model: {best_model}\")\n",
    "\n",
    "# Save the best model and tokenizer to the final directory\n",
    "final_dir = config['paths']['model_dirs']['final']\n",
    "if best_model == \"RoBERTa\":\n",
    "    trainer_roberta.model.save_pretrained(final_dir)\n",
    "    tokenizer_roberta.save_pretrained(final_dir)\n",
    "else:\n",
    "    trainer.model.save_pretrained(final_dir)\n",
    "    tokenizer.save_pretrained(final_dir)\n",
    "\n",
    "logging.info(f\"Saved {best_model} as final model at {final_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Validation F1 Comparison ────────────────────────────────────────────\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Gather F1 scores\n",
    "f1_scores = [\n",
    "    metrics['eval_f1'],\n",
    "    roberta_metrics['eval_f1']\n",
    "]\n",
    "models = ['BERT‑base', 'RoBERTa‑base']\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "bars = ax.bar(models, f1_scores, color=['#4F81BD', '#C0504D'])\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_ylabel('Validation F1')\n",
    "ax.set_title('BERT vs RoBERTa Validation F1')\n",
    "\n",
    "# Annotate values\n",
    "for bar, score in zip(bars, f1_scores):\n",
    "    ax.text(\n",
    "        bar.get_x() + bar.get_width()/2,\n",
    "        score + 0.02,\n",
    "        f\"{score:.3f}\",\n",
    "        ha='center',\n",
    "        va='bottom'\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Evaluation on Test Set\n",
    "\n",
    "I now evaluate the selected final model on the held-out test set to report our definitive performance metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test dataset for evaluation\n",
    "from datasets import Dataset\n",
    "\n",
    "tokenizer_final = tokenizer_roberta if best_model==\"RoBERTa\" else tokenizer\n",
    "trainer_final   = trainer_roberta if best_model==\"RoBERTa\" else trainer\n",
    "\n",
    "test_enc = tokenizer_final(\n",
    "    test_df['text'].tolist(),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=config['training']['max_length']['bert_roberta']\n",
    ")\n",
    "test_dataset = Dataset.from_dict({\n",
    "    **test_enc,\n",
    "    'labels': [config['model']['label_mapping'][l] for l in test_df['label']]\n",
    "})\n",
    "\n",
    "# Evaluate on test set\n",
    "test_metrics = trainer_final.evaluate(test_dataset=test_dataset)\n",
    "print(\"Test Set Performance:\", test_metrics)\n",
    "logging.info(f\"Test set performance: {test_metrics}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Confusion Matrix on Test Set ───────────────────────────────────────\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Get true & predicted labels\n",
    "y_true = [config['model']['label_mapping'][l] for l in test_df['label']]\n",
    "preds = trainer_final.predict(test_dataset)\n",
    "y_pred = preds.predictions.argmax(axis=-1)\n",
    "\n",
    "# Build matrix & label names\n",
    "labels = list(config['model']['label_mapping'].keys())\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=labels,\n",
    "    yticklabels=labels,\n",
    "    ax=ax\n",
    ")\n",
    "ax.set_xlabel('Predicted Label')\n",
    "ax.set_ylabel('True Label')\n",
    "ax.set_title('Test‑Set Confusion Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Analysis\n",
    "\n",
    "Let’s visualize the confusion matrix and ROC/PR curves on the test set to understand where my model excels and where it makes mistakes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer_final' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m viz\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Get raw predictions on test\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m pred_out \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer_final\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(test_dataset)\n\u001b[0;32m      5\u001b[0m logits \u001b[38;5;241m=\u001b[39m pred_out\u001b[38;5;241m.\u001b[39mpredictions\n\u001b[0;32m      6\u001b[0m y_true \u001b[38;5;241m=\u001b[39m [config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_mapping\u001b[39m\u001b[38;5;124m'\u001b[39m][l] \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainer_final' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from utils import viz\n",
    "# Get raw predictions on test\n",
    "pred_out = trainer_final.predict(test_dataset)\n",
    "logits = pred_out.predictions\n",
    "y_true = [config['model']['label_mapping'][l] for l in test_df['label']]\n",
    "y_pred = logits.argmax(axis=1)\n",
    "y_prob = torch.softmax(torch.tensor(logits), dim=1).numpy()\n",
    "\n",
    "# Plot & save confusion matrix (normalized)\n",
    "viz.plot_confusion_matrix(\n",
    "    y_true, y_pred,\n",
    "    labels=list(config['model']['label_mapping'].keys()),\n",
    "    normalize=True,\n",
    "    save_path=\"diagrams/confusion_matrix.png\"\n",
    ")\n",
    "\n",
    "# Plot & save ROC curves\n",
    "viz.plot_roc_curves(\n",
    "    y_true, y_prob,\n",
    "    class_names=list(config['model']['label_mapping'].keys()),\n",
    "    save_path=\"diagrams/roc_curves.png\"\n",
    ")\n",
    "\n",
    "# Plot & save Precision-Recall curves\n",
    "viz.plot_pr_curves(\n",
    "    y_true, y_prob,\n",
    "    class_names=list(config['model']['label_mapping'].keys()),\n",
    "    save_path=\"diagrams/pr_curves.png\"\n",
    ")\n",
    "\n",
    "print(\"Saved confusion matrix, ROC curves, and PR curves to diagrams/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "\n",
    "To understand our model’s mistakes, let’s look at a few examples where the predicted label differs from the true label (especially between AI‑generated and AI‑paraphrased).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Find misclassified indices\n",
    "mis_idx = np.where(y_pred != y_true)[0]\n",
    "print(f\"Total misclassifications: {len(mis_idx)} ({len(mis_idx)/len(y_true)*100:.2f}% of test set)\")\n",
    "\n",
    "# Display first 5 misclassified examples\n",
    "for idx in mis_idx[:5]:\n",
    "    true_label = list(config['model']['label_mapping'].keys())[y_true[idx]]\n",
    "    pred_label = list(config['model']['label_mapping'].keys())[y_pred[idx]]\n",
    "    print(f\"\\nExample {idx}: True = {true_label}, Predicted = {pred_label}\")\n",
    "    print(test_df.iloc[idx]['text'][:200] + \"...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainability with LIME\n",
    "\n",
    "I use LIME to interpret the model’s prediction for a specific example, showing which words contributed most to the decision.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import dashboard_utils\n",
    "\n",
    "# Choose an example (first misclassified if available, else index 0)\n",
    "idx = mis_idx[0] if len(mis_idx) > 0 else 0\n",
    "sample_text = test_df.iloc[idx]['text']\n",
    "true_label = test_df.iloc[idx]['label']\n",
    "print(f\"Explaining example {idx} (True label: {true_label})\\n\")\n",
    "print(sample_text[:300] + \"...\\n\")\n",
    "\n",
    "# Generate LIME explanation\n",
    "expl = dashboard_utils.explain_prediction(\n",
    "    sample_text,\n",
    "    tokenizer_roberta if best_model==\"RoBERTa\" else tokenizer,\n",
    "    trainer_roberta.model if best_model==\"RoBERTa\" else trainer.model,\n",
    "    num_features=6\n",
    ")\n",
    "\n",
    "# Display the top contributing words\n",
    "print(\"Top contributing words and weights:\")\n",
    "for word, weight in expl:\n",
    "    print(f\"{word:>15}: {weight:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "- My **RoBERTa-base** detector achieves about **91% accuracy** (macro F1 ~0.91) on the test set.  \n",
    "- It almost perfectly distinguishes human text (recall > 99%), with most errors occurring between AI-generated and AI-paraphrased classes.  \n",
    "- BERT was a strong baseline but slightly behind RoBERTa, confirming the benefits of RoBERTa’s pretraining improvements.  \n",
    "- LIME explanations provide insight into which words drive each prediction, aiding model interpretability.\n",
    "\n",
    "With the model finalized, I'm ready to deploy it in interactive dashboards (next notebooks).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyProject (Conda)",
   "language": "python",
   "name": "myproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
