{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotly Dash Dashboard\n",
    "\n",
    "This dashboard allows interactive exploration of the AI Text Detector results. It has three tabs for:\n",
    "1. **EDA** â€“ Visualizing data distributions (class balance, article length, etc.).\n",
    "2. **Evaluation** â€“ Showing model performance metrics (confusion matrix, ROC curves).\n",
    "3. **Inference** â€“ Providing an interface to input new text and get predictions with explanations.\n",
    "\n",
    "Run this app to launch the dashboard locally and interact with the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new cwd: c:\\Testing\\Final_Year_Project\\AI-Text-Detection-Tool\n"
     ]
    }
   ],
   "source": [
    "# move up one level so that works\n",
    "import os\n",
    "os.chdir(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "print(\"new cwd:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dash\n",
    "from dash import dcc, html\n",
    "import base64, logging\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import logging\n",
    "from utils import dashboard_utils  # for LIME explanations\n",
    "import pandas as pd\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Image not found, skipping: C:\\Testing\\Final_Year_Project\\AI-Text-Detection-Tool\\diagrams\\confusion_matrix.png\n",
      "WARNING:root:Image not found, skipping: C:\\Testing\\Final_Year_Project\\AI-Text-Detection-Tool\\diagrams\\roc_curves.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize Dash app\n",
    "app = dash.Dash(__name__)\n",
    "app.title = \"AI Text Detector Dashboard\"\n",
    "\n",
    "# â€”â€”â€” 1) Autoâ€‘locate your diagrams/ directory â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "def find_diagrams_dir(start: Path = Path.cwd(), marker: str = \"diagrams\") -> Optional[Path]:\n",
    "    \"\"\"\n",
    "    Walk upward from `start` through its parents until you find a folder named `marker`.\n",
    "    Return the Path to that folder, or None if not found.\n",
    "    \"\"\"\n",
    "    for folder in (start, *start.parents):\n",
    "        candidate = folder / marker\n",
    "        if candidate.is_dir():\n",
    "            return candidate\n",
    "    logging.warning(f\"Couldnâ€™t locate a '{marker}/' directory under {start}\")\n",
    "    return None\n",
    "\n",
    "DIAGRAMS_DIR = find_diagrams_dir()\n",
    "\n",
    "# â€”â€”â€” 2) Helper to read & encode image files as base64 URIs, safely â€”â€”â€”â€”â€”â€”\n",
    "def encode_image(filename: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Given a filename (e.g. \"class_distribution.png\"), look in DIAGRAMS_DIR,\n",
    "    read it if present, and return a data URI. If missing, log & return None.\n",
    "    \"\"\"\n",
    "    if DIAGRAMS_DIR is None:\n",
    "        return None\n",
    "\n",
    "    img_path = (DIAGRAMS_DIR / filename).resolve()\n",
    "    if not img_path.exists():\n",
    "        logging.warning(f\"Image not found, skipping: {img_path}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        raw = img_path.read_bytes()\n",
    "        b64 = base64.b64encode(raw).decode(\"utf-8\")\n",
    "        return f\"data:image/png;base64,{b64}\"\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Failed to encode {img_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# â€”â€”â€” 3) Load preâ€‘generated plots (wonâ€™t crash if missing) â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "class_dist_img  = encode_image(\"class_distribution.png\")\n",
    "length_dist_img = encode_image(\"length_distribution.png\")\n",
    "conf_matrix_img = encode_image(\"confusion_matrix.png\")\n",
    "roc_curves_img  = encode_image(\"roc_curves.png\")\n",
    "\n",
    "# â€”â€”â€” 4) Define app layout â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "app.layout = html.Div([\n",
    "    html.H1(\n",
    "        \"AIâ€‘Generated Text Detection Dashboard\",\n",
    "        style={\"textAlign\": \"center\", \"marginBottom\": \"1em\"}\n",
    "    ),\n",
    "    dcc.Tabs(id=\"tabs\", value=\"tab-eda\", children=[\n",
    "        dcc.Tab(label=\"EDA\",        value=\"tab-eda\"),\n",
    "        dcc.Tab(label=\"Evaluation\", value=\"tab-eval\"),\n",
    "        dcc.Tab(label=\"Inference\",  value=\"tab-inf\"),\n",
    "    ]),\n",
    "    html.Div(id=\"tab-content\")\n",
    "])\n",
    "\n",
    "# (Callbacks will go here in later cellsâ€¦)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/trends_by_year.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trends_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/trends_by_year.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Melt into long format for plotting\u001b[39;00m\n\u001b[0;32m      4\u001b[0m records \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\micha\\.conda\\envs\\myenv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\micha\\.conda\\envs\\myenv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\micha\\.conda\\envs\\myenv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\micha\\.conda\\envs\\myenv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\micha\\.conda\\envs\\myenv\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/trends_by_year.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "trends_df = pd.read_csv('data/trends_by_year.csv')\n",
    "\n",
    "# Melt into long format for plotting\n",
    "records = []\n",
    "for _, row in trends_df.iterrows():\n",
    "    year = int(row['year'])\n",
    "    records.extend([\n",
    "        {'year': year, 'content_type': 'Human-written',  'count': row['Human-written'],     'percentage': row['human_percent']},\n",
    "        {'year': year, 'content_type': 'AI-paraphrased', 'count': row['AI-paraphrased'],   'percentage': row['ai_paraphrased_percent']},\n",
    "        {'year': year, 'content_type': 'AI-generated',  'count': row['AI-generated'],    'percentage': row['ai_generated_percent']},\n",
    "    ])\n",
    "trends_long = pd.DataFrame(records)\n",
    "\n",
    "# Line chart of proportions over time\n",
    "fig_line = px.line(\n",
    "    trends_long, x='year', y='percentage', color='content_type', markers=True,\n",
    "    labels={'percentage':'Proportion','content_type':'Type','year':'Year'},\n",
    ")\n",
    "fig_line.update_layout(\n",
    "    title=\"AI vs Human Content Over Time\",\n",
    "    yaxis_tickformat='%'\n",
    ")\n",
    "fig_line.update_traces(\n",
    "    hovertemplate=\"Year %{x}<br>%{legendgroup}: %{y:.1%} (%{customdata} articles)\",\n",
    "    customdata=trends_long['count']\n",
    ")\n",
    "\n",
    "# Animated bar chart\n",
    "fig_bar = px.bar(\n",
    "    trends_long, x='content_type', y='percentage', color='content_type',\n",
    "    animation_frame='year', animation_group='content_type', range_y=[0,1],\n",
    "    category_orders={'content_type':['Human-written','AI-paraphrased','AI-generated']},\n",
    "    labels={'percentage':'Proportion','content_type':'Type','year':'Year'}\n",
    ")\n",
    "fig_bar.update_layout(\n",
    "    title=\"Content-Type Timeline (2015â€“2025)\",\n",
    "    yaxis_tickformat='%'\n",
    ")\n",
    "fig_bar.update_traces(\n",
    "    hovertemplate=\"%{x}: %{y:.1%} (%{customdata} articles)\",\n",
    "    customdata=trends_long['count']\n",
    ")\n",
    "\n",
    "# Initialize Dash app\n",
    "app = dash.Dash(__name__)\n",
    "app.layout = html.Div([\n",
    "    dcc.Tabs(id=\"tabs\", value=\"tab-trends\", children=[\n",
    "        dcc.Tab(label=\"Trends\", value=\"tab-trends\"),\n",
    "        dcc.Tab(label=\"Evaluation\", value=\"tab-eval\"),\n",
    "        # ... other tabs ...\n",
    "    ]),\n",
    "    html.Div(id=\"tab-content\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model & tokenizer once\n",
    "model_path = \"diagrams/final_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "model.eval()\n",
    "\n",
    "# Class names in the correct order\n",
    "label_names = [\"Human-written\", \"AI-paraphrased\", \"AI-generated\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dash.dependencies import Input, Output\n",
    "\n",
    "@app.callback(Output(\"tab-content\", \"children\"), Input(\"tabs\", \"value\"))\n",
    "def render_tab_content(tab):\n",
    "    if tab == \"tab-eda\":\n",
    "        return html.Div([\n",
    "            html.H3(\"Exploratory Data Analysis\", style={\"textAlign\": \"center\", \"marginTop\": \"1em\"}),\n",
    "            html.Img(src=class_dist_img, style={\"width\": \"45%\", \"display\": \"inline-block\", \"padding\": \"1em\"}),\n",
    "            html.Img(src=length_dist_img, style={\"width\": \"45%\", \"display\": \"inline-block\", \"padding\": \"1em\"}),\n",
    "            html.P(\n",
    "                \"The dataset is fairly balanced across classes. \"\n",
    "                \"Human-written texts are generally longer than AI-generated or AI-paraphrased ones.\",\n",
    "                style={\"textAlign\": \"center\", \"fontStyle\": \"italic\", \"marginTop\": \"0.5em\"}\n",
    "            )\n",
    "        ])\n",
    "    elif tab == \"tab-eval\":\n",
    "        # Evaluation tab: show confusion matrix and ROC curves\n",
    "        return html.Div([\n",
    "            html.H3(\"Model Evaluation\", style={\"textAlign\": \"center\", \"marginTop\": \"1em\"}),\n",
    "            html.Img(src=conf_matrix_img, style={\"width\": \"40%\", \"display\": \"inline-block\", \"padding\": \"1em\"}),\n",
    "            html.Img(src=roc_curves_img, style={\"width\": \"50%\", \"display\": \"inline-block\", \"padding\": \"1em\"}),\n",
    "            html.P(\n",
    "                \"Overall accuracy ~91%. The model excels at identifying human-written text (near 99% recall) \"\n",
    "                \"and mostly confuses paraphrased vs directly AI-generated text.\",\n",
    "                style={\"textAlign\": \"center\", \"fontStyle\": \"italic\", \"marginTop\": \"0.5em\"}\n",
    "            )\n",
    "        ])\n",
    "    elif tab == \"tab-inf\":\n",
    "        # Inference tab: textarea + button + placeholder for results\n",
    "        return html.Div([\n",
    "            html.H3(\"Try the Detector\", style={\"textAlign\": \"center\", \"marginTop\": \"1em\"}),\n",
    "            dcc.Textarea(\n",
    "                id=\"input-text\",\n",
    "                placeholder=\"Enter article text here...\",\n",
    "                style={\"width\": \"80%\", \"height\": \"100px\"}\n",
    "            ),\n",
    "            html.Br(),\n",
    "            html.Button(\"Detect\", id=\"detect-button\", n_clicks=0, style={\"marginTop\": \"0.5em\"}),\n",
    "            html.Div(id=\"result-output\", style={\"marginTop\": \"1em\"})\n",
    "        ])\n",
    "    elif tab == \"tab-trends\":\n",
    "        return html.Div([\n",
    "            html.H3(\"AI Content Trends (2015â€“2025)\",\n",
    "                    style={\"textAlign\":\"center\",\"marginTop\":\"1em\"}),\n",
    "            dcc.Graph(id=\"trend-line\",\n",
    "                      figure=fig_line,\n",
    "                      config={\"displayModeBar\": False}),\n",
    "            dcc.Graph(id=\"trend-bar\",\n",
    "                      figure=fig_bar,\n",
    "                      config={\"displayModeBar\": False}),\n",
    "            html.P(\"Use the slider â–¶ to animate year-by-year changes. \"\n",
    "                   \"Hover for exact percentages and counts.\",\n",
    "                   style={\"textAlign\":\"center\",\n",
    "                          \"fontStyle\":\"italic\",\n",
    "                          \"marginTop\":\"0.5em\"})\n",
    "        ])\n",
    "    return html.Div() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dash.dependencies import Input, Output\n",
    "\n",
    "@app.callback(\n",
    "    Output(\"result-output\", \"children\"),\n",
    "    [Input(\"detect-button\", \"n_clicks\"), Input(\"input-text\", \"value\")]\n",
    ")\n",
    "def run_detection(n_clicks, input_text):\n",
    "    if not n_clicks or not input_text:\n",
    "        return \"\"\n",
    "    # 1) Tokenize & predict\n",
    "    tokens = tokenizer(\n",
    "        input_text, return_tensors=\"pt\",\n",
    "        truncation=True, padding=True, max_length=512\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        logits = model(**tokens).logits\n",
    "        probs = torch.softmax(logits, dim=1)[0].cpu().numpy()\n",
    "    pred_idx = int(np.argmax(probs))\n",
    "    pred_label = label_names[pred_idx]\n",
    "    confidence = probs[pred_idx]\n",
    "    logging.info(f\"Predicted {pred_label} ({confidence:.3f})\")\n",
    "\n",
    "    # 2) Get LIME explanation (top 6 words)\n",
    "    explanation = dashboard_utils.explain_prediction(\n",
    "        input_text, tokenizer, model, num_features=6\n",
    "    )\n",
    "    # Build a lookup of wordâ†’weight\n",
    "    weights = {w.lower(): wt for w, wt in explanation}\n",
    "    max_w = max(abs(wt) for wt in weights.values()) if weights else 1.0\n",
    "\n",
    "    # 3) Render probability bars\n",
    "    prob_rows = []\n",
    "    for i, name in enumerate(label_names):\n",
    "        pct = probs[i] * 100\n",
    "        bar = html.Div(style={\n",
    "            \"width\": f\"{pct}%\", \"height\": \"8px\", \"backgroundColor\": \"#4c8bf5\"\n",
    "        })\n",
    "        prob_rows.append(\n",
    "            html.Div([bar, html.Span(f\" {name}: {pct:.1f}%\")],\n",
    "                     style={\"display\":\"flex\",\"alignItems\":\"center\",\"margin\":\"4px 0\"})\n",
    "        )\n",
    "\n",
    "    # 4) Highlight important words\n",
    "    nodes = []\n",
    "    for word in input_text.split():\n",
    "        key = word.strip(\".,!?;:\").lower()\n",
    "        if key in weights:\n",
    "            w = weights[key]\n",
    "            opacity = min(abs(w)/max_w,1.0)\n",
    "            color = f\"rgba(255,165,0,{opacity:.2f})\"\n",
    "            nodes.append(html.Span(word+\" \",\n",
    "                                   style={\"backgroundColor\": color},\n",
    "                                   title=f\"Weight: {w:+.2f}\"))\n",
    "        else:\n",
    "            nodes.append(html.Span(word+\" \"))\n",
    "\n",
    "    # 5) Compose output\n",
    "    return html.Div([\n",
    "        html.H4(f\"Prediction: {pred_label}\", style={\"textAlign\":\"center\"}),\n",
    "        html.P(f\"Confidence: {confidence*100:.2f}%\", style={\"textAlign\":\"center\"}),\n",
    "        html.Div(prob_rows, style={\"margin\":\"0.5em 0\"}),\n",
    "        html.P(nodes, style={\"lineHeight\":\"1.8em\"})\n",
    "    ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1ad3700e0a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    app.run_server(debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dashboard Design & Integration\n",
    "\n",
    "- **EDA Tab**: validates data assumptions (class balance, length differences).\n",
    "- **Evaluation Tab**: shows ~91% accuracy, highlights AI-para vs AI-gen confusion.\n",
    "- **Inference Tab**: live text input with LIME highlights (hover shows weights).\n",
    "\n",
    "This dashboard is ideal for presentationsâ€”walk examiners through data, model performance, and explainability in a unified interface.\n",
    "\n",
    "Alongside this, the **React** app and **Chrome extension** offer real-world workflows:\n",
    "- React: single/batch analysis with an intuitive sidebar, dark mode, and drag-drop.\n",
    "- Extension: scan any web page in-place, highlighting the exact cues that triggered the model.\n",
    "\n",
    "ðŸ”‘ **Note:** Keep the FastAPI backend (`api_server.py`) running.  \n",
    "Dash can run standalone (it loads the model internally), but for full parity use the API.\n",
    "\n",
    "> The modular design means you can upgrade each component independentlyâ€”good software practice for extending this project further.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyProject (Conda)",
   "language": "python",
   "name": "myproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
