{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Longformer Model for Extended Context (Optional)\n",
    "\n",
    "In cases where articles exceed 512 tokens, BERT/RoBERTa truncates important content. **Longformer** can handle up to 4096 tokens using sparse attention, making it ideal for full‑length articles.\n",
    "\n",
    "Training Longformer end‑to‑end is resource‑intensive, so here I provide the code scaffold without executing a full training run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import yaml\n",
    "\n",
    "# Load config to get max_length for longformer\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Initialize Longformer tokenizer & model\n",
    "tokenizer_long = AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "model_long     = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"allenai/longformer-base-4096\",\n",
    "    num_labels=3\n",
    ")\n",
    "model_long.eval()\n",
    "\n",
    "# Demonstrate tokenization on a small sample\n",
    "sample_texts = train_df['text'].head(2).tolist()\n",
    "tokens = tokenizer_long(\n",
    "    sample_texts,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=config['training']['max_length']['longformer']\n",
    ")\n",
    "print(\"Tokenized length for first sample:\", len(tokens['input_ids'][0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Longformer (Not Executed)\n",
    "\n",
    "Normally, I would now fine‑tune Longformer similarly to BERT/RoBERTa:\n",
    "1. Tokenize the full train/val sets with `max_length=4096`.  \n",
    "2. Create `Dataset` objects with labels.  \n",
    "3. Set up `TrainingArguments` (reduce `batch_size` to 4 due to memory).  \n",
    "4. Instantiate `CustomTrainer` with `model_long`, metrics, and callbacks.  \n",
    "5. Call `trainer_long.train()`.\n",
    "\n",
    "However, training with 4,096‑token sequences is very slow (≈3-4× per‑epoch time of RoBERTa). To conserve resources, we are **not** running a full Longformer training here.\n",
    "\n",
    "```python\n",
    "# (Pseudo‑code – do not run)\n",
    "# train_enc_long = tokenizer_long(train_texts, padding=True, truncation=True, max_length=4096)\n",
    "# val_enc_long   = tokenizer_long(val_texts, padding=True, truncation=True, max_length=4096)\n",
    "# train_dataset_long = Dataset.from_dict({...})\n",
    "# training_args_long = TrainingArguments(output_dir=..., num_train_epochs=3, per_device_train_batch_size=4, ...)\n",
    "# trainer_long = CustomTrainer(model_long, args=training_args_long, ...)\n",
    "# trainer_long.train()\n",
    "```\n",
    "While Longformer may improve recall on very long articles, our dataset’s median length (~250 words) is well within 512 tokens, so RoBERTa suffices for deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "I provided the full scaffold to fine‑tune Longformer for extended contexts, but did not execute a complete training run to conserve resources. Given our dataset’s median length (~250 words), a 512‑token model already captures most content. My final chosen model remains **RoBERTa‑base**, which I'm fine‑tuned and saved for deployment.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyProject (Conda)",
   "language": "python",
   "name": "myproject"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
